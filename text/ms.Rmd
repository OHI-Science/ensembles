```{r, echo=FALSE}
# Values to use in the paper:
load("values.rda") # generated by `../analysis/9-output-values.R`

# Includes: mean_sim, mean_ram, slope_sim, auc_sim, ram_stocks_n

# mean_sim, mean_ram, and slope_sim have the following list elements:
# > names(mean_sim)
# "mach_vs_ind_mare_fold" "ind_corr_range" "mach_corr_range" "ind_mare_range"  
# "mach_mare_range" "ensemble_mre_range" "ind_mre_range"
 
# auc_sim has the following list elements:
# > names(auc_sim)
# "mach_range" "ind_range" 
```

<!--
 The full title of the paper, with two alternatives. Keep titles short and simple.
• The full names of all authors.
• The name(s) and address(es) of the institution(s) at which the work was carried out. If the present address of any author is different from the above, give it as a footnote.
• The name, address, telephone, fax and e-mail contacts of the author to whom all correspondence and proofs should be sent.
• A suggested running title of not more than 40 characters, including spaces.
-->

\begin{Large}
\noindent
Improving estimates of population status and trend with superensemble models
\end{Large}

\bigskip

\noindent
Sean C. Anderson^1\*^, 
Andrew B. Cooper^1^,
Olaf P. Jensen^2^, 
C\'{o}il\'{i}n Minto^3^, 
James T. Thorson^4^,
Jessica C. Walsh^1^,
Jamie Afflerbach^5^, 
Mark Dickey-Collas^6,7^, 
Kristin M. Kleisner^8^, 
Catherine Longo^9^, 
Giacomo Chato Osio^10^, 
Daniel Ovando^11^, 
Iago Mosqueira^10^,
Andrew A. Rosenberg^12^, 
Elizabeth R. Selig^13^

\bigskip

\noindent
^1^School of Resource and Environmental Management,
Simon Fraser University, Burnaby, BC, V5A 1S6, Canada

\noindent
^2^Institute of Marine & Coastal Sciences, Rutgers University, 
71 Dudley Road, New Brunswick, NJ, 08901-8525, USA

\noindent
^3^Marine and Freshwater Research Centre, Galway-Mayo Institute of Technology, 
Galway, 00000, Ireland

\noindent
^4^Fisheries Resource Analysis and Monitoring Division, Northwest Fisheries Science Center, National Marine Fisheries Service, National Oceanographic and Atmospheric Administration, 2725 Montlake Boulevard E., Seattle, WA, 98112, USA

\noindent
^5^National Center for Ecological Analysis and Synthesis, University of
California Santa Barbara, 735 State Street, Suite 300, Santa Barbara, CA, 93103,
USA

\noindent
^6^International Council for the Exploration of the Sea, 
H.C. Andersens Boulevard 44-46, DK 1553, Copenhagen, Denmark

\noindent
^7^DTU Aqua National Institute of Aquatic Resources, Technical University of Denmark (DTU), Jægersborg Alle 1, 2920 Charlottenlund, Denmark

\noindent
^8^Ecosystem Assessment Program, Northeast Fisheries Science Center, National Marine Fisheries Service, National Oceanographic and Atmospheric Administration, 166 Water St., Woods Hole, MA, 02543, USA

\noindent
^9^Marine Stewarship Council, Marine House, 1 Snow Hill, London, EC1A 2DH, United Kingdom

\noindent
^10^European Commission, Joint Research Centre, Institute for the Protection and
Security of the Citizen, Maritime Affairs Unit G03, Via E. Fermi 2749, 21027
Ispra (VA), Italy

\noindent
^11^Bren School of Environmental Science and Management, 
University of California Santa Barbara, 
Santa Barbara, CA, 93106-5131, USA

\noindent
^12^Union of Concerned Scientists, Cambridge, MA, USA

\noindent
^13^Conservation International, Arlington, VA, USA


\noindent
^\*^Corresponding author, present address: School of Aquatic and Fishery Sciences, Box 355020, University of Washington, Seattle, WA  98195, USA
E-mail: sandrsn@uw.edu

\noindent
Running head: Superensembles of population status

\clearpage
<!--up to 250 words-->

\bigskip
\noindent
\textbf{Abstract:} Ecological resource managers must often reconcile multiple conflicting estimates of population status and trend. One solution is to average these predictions in an ensemble, but this approach ignores covariance between models and may not optimally reduce bias and improve accuracy. Superensemble models, commonly used in climate and weather forecasting, may provide a superior solution. Superensembles use the predictions from multiple models as covariates in an additional statistical model fit to known data. Here we evaluate the potential for ensemble and superensemble models ('ensemble methods') to improve estimates of fish population status and trend. We fit four data-limited and widely applicable models of population and exploitation dynamics and combine the models' estimates of the mean and recent trend in biomass relative to the equilibrium biomass at maximum sustainable yield ($B/B_\mathrm{MSY}$) with an ensemble average and three superensembles: a linear model, a random forest, and a boosted regression tree. We built our models on a simulated dataset of 5760 stocks and tested them with cross-validation and against a global database of `r ram_stocks_n` stock assessments. We found that ensemble methods substantially improved estimates of population status and trend. Machine-learning superensembles (random forest and boosted regression trees) performed the best for estimating population status: accuracy improved `r mean_sim$mach_vs_ind_mare_fold`%, rank-order correlation between predicted and true status improved from `r mean_sim$ind_corr_range` to `r mean_sim$mach_corr_range`, and bias (median proportional error) declined from `r mean_sim$ind_mre_range` to `r mean_sim$mach_mre_range`. We found similar improvements when predicting the trend of status and when applying the simulation-trained superensembles to catch data for global fish stocks. Ensemble methods have the potential to improve estimates of population status and trends for a wide variety of plant and animal species. However, they must be tested, formed from a diverse set of accurate models, and built on a dataset representative of the populations to which they are applied.

\bigskip

\noindent 
Keywords: data-limited fisheries, ensemble methods, multi-model averaging, population dynamics, sustainable resource management

\tableofcontents

# Introduction

Population status and trend are two of the most fundamental values to quantify in conservation biology [e.g. @mace2008; @iucn2015]. These properties are key to everything from global assessments of extinction risk [e.g. @stuart2004; @schipper2008] to assessing the sustainability of hunting [e.g. @fa2002] and fishing exploitation levels [e.g. @worm2009; @juan-jorda2011]. However, conservation and ecological resource managers are often faced with reconciling multiple uncertain and potentially conflicting estimates of status and trend [e.g. @brodziak2010; @branch2011; @deroba2015]. Some models may suggest a population is at risk and declining in abundance while others may suggest it is not at risk and stable.

One solution is to take the average or weighted average of an ensemble of model predictions. Such ensembles are typically more accurate and less biased than individual model estimates and can integrate uncertainty in model structure, initial conditions, and parameter estimation [@dietterich2000; @araujo2007]. This approach forms the basis of many machine learning methods [e.g. @dietterich2000], has helped reconcile climate forecasts from dozens of models [e.g. @murphy2004; @tebaldi2007; @ipcc2013], and even improved early warning signs of malaria outbreaks [@thomson2006]. In ecology, ensemble methods are frequently used to improve species distribution modelling [e.g. @araujo2007; @breiner2015] and indeed have been used to combine estimates of population status and trend [e.g. @brodziak2010].

Whereas averages or weighted averages may improve predictions over any single model, they may not optimally leverage available data. The best prediction does not necessarily lie in the middle of multiple model predictions, some models may perform better than others in certain conditions, and the covariance between models may contain information that can improve predictive accuracy. We can exploit these characteristics by using the predictions from a group of models as inputs into a separate statistical model. This technique, sometimes called superensemble modelling [@krishnamurti1999], is common in climate and weather forecasting [e.g. @yun2005; @mote2015]. The superensemble is fit to a training dataset where outcomes are well known and used to predict on a dataset of interest (Fig. \ref{didactic}). For example, @krishnamurti1999 combined predictions of wind and precipitation in Asian monsoons via a superensemble regression fit to observed data. Their superensemble was considerably more accurate than any individual prediction or weighted average of predictions. 

In fisheries science, the commonly used operational models for determining status and trend of exploited fish populations are stock assessments, i.e., population models coupled to an observation model, that incorporate all appropriate data (e.g. catches, size and age distributions, surveys, and tagging information) to quantify values such as the biomass of a stock that can produce maximum sustainable yield ($B/B_\mathrm{MSY}$) [@hilborn1992]. However, the broad range of data required to conduct these stock assessments are not available for the majority of fish populations, including those of conservation concern and of economic interest to fisheries [@fao2014]. Therefore, a number of models have been proposed to assess $B/B_\mathrm{MSY}$ based on the limited data available for the majority of fish stocks: (1) a time series of the total biomass of catch and (2) a basic understanding of population productivity [e.g. @vasconcellos2005; @martell2013]. Recently, @rosenberg2014 investigated the performance of four data-limited models through a large-scale simulation experiment. Three of these models are based on Schaefer (logistic) biomass dynamics and one is an empirical model fitted to more data-rich stock assessment output. The four models frequently disagreed about population status, no one model had strong performance across all fish stocks, and some models performed better than others depending on circumstances (e.g. Fig. \ref{motivate}).

Here, we estimate population status and trend of exploited fish populations using ensembles and superensembles (collectively 'ensemble methods') of these four data-limited models. We apply four ensemble-method approaches of increasing complexity to both simulated and real-world fish stocks and compare their predictive performance against each other and the individual models.

<!--
 TODO: It might be good to throw in a single sentence here describing the 4 models, or at least listing them and maybe a couple key differences among them.-->

# Methods

We first developed and tested ensemble methods on a fully factorial simulated dataset [@rosenberg2014] with known status. These published simulated data included the following scenarios: three fish life histories: small pelagic, demersal, and large pelagic; three levels of initial biomass depletion compared to carrying capacity: biomass at 100%, 70%, and 40% of carrying capacity; and four exploitation settings: (1) a constant exploitation rate, (2) an exploitation rate coupled with biomass to mimic an open-access single-species fishery, (3) a scenario where exploitation rate increased continuously, and (4) a 'roller-coaster' scenario where the exploitation rate increased and then decreased. Process noise (recruitment variability; i.e. unexplained variability in population dynamics) was introduced to the models at two magnitudes in log space, $N(0, 0.2^2)$ and $N(0, 0.6^2)$, and was either uncorrelated through time or had a first-order autoregressive correlation of $0.6$. The simulation also included a scenario with $N(0, 0.2^2)$ measurement error around log(catch) and one scenario without measurement error.
@rosenberg2014 ran ten iterations for each combination of factors adding stochastic draws of recruitment and catch-recording variability each time to generate a total of 5760 stocks. 

<!--TODO add back when identity public: The simulation models were built in the package `FLR` [@kell2007, <http://flr-project.org>] for the statistical software \textsf{R} [@r2015]. -->
<!--Code to generate the simulations is available at <https://github.com/iagomosqueira/stockStatusFAO>.-->

We also tested our ensemble methods on the RAM Legacy Stock Assessment Database [@ricard2012] --- a compilation of stock-assessment output from hundreds of exploited marine populations around the world. Our analysis of the stock-assessment database was based on version 2.5; after removing stocks for which at least one of the individual models described below did not converge (121), this database included `r ram_stocks_n` stocks.

## Individual models of population status

We fit four individual data-limited models to estimate $B/B_\mathrm{MSY}$. Three of the models are mechanistic and based generally on Schaefer biomass dynamics [@schaefer1954] of the form

$$\hat{B}_{t+1} = B_t + r B_t \left(1 - B_t / B_0 \right) - C_t,$$

\noindent
where $\hat{B}_{t+1}$ represents predicted biomass at time $t$ plus one year, $B_t$ represents biomass at time $t$, $r$ represents the intrinsic population growth rate, $B_0$ represents unfished biomass or carrying capacity $K$, and $C$ represents catch. The fourth model is an empirically derived model based on the RAM Legacy Stock Assessment Database. @rosenberg2014 provide a full background on these four methods and code to fit the models. In summary:

<!--and code to fit all the models is available in an accompanying package `datalimited` (TODO: make public?) for the statistical software \textsf{R}. In summary:-->

* *CMSY* (catch-MSY) implements a stock-reduction analysis with Schaefer biomass dynamics [@martell2013]. It requires a prior distribution for $r$ and assigns a prior to the relative proportion of biomass at the end of the time series compared to unfished biomass (depletion) based on the percentage of maximum catch at the end of the time series. The version of the model used in @rosenberg2014 was modified from @martell2013 to generate biomass trends from all viable $r$-$K$ pairs and produce an estimate of $B/B_\mathrm{MSY}$ from the median trend.

* *COM-SIR* (catch-only-model with sampling-importance-resampling) is a coupled harvest-dynamics model [@vasconcellos2005]. Biomass is assumed to follow a Schaefer model and harvest dynamics are assumed to follow a logistic model. The model is fit with a sampling-importance-sampling algorithm [@rosenberg2014].

* *SSCOM* (state-space catch-only model) is a hierarchical model that, similar to COM-SIR, is based on a coupled harvest-dynamics model [@thorson2013]. SSCOM estimates unobserved dynamics in both fishing effort and the fished population based on a catch time series and priors on $r$, the maximum rate of increase of fishing effort, and the magnitude of various forms of stochasticity. The model is fit in a Bayesian state-space framework to integrate across three forms of stochasticity: variation in effort, population dynamics, and fishing efficiency [@thorson2013].

* *mPRM* (modified panel regression model) is a modified version of the panel-regression model from @costello2012. Unlike the other models, mPRM is empirical and not mechanistic --- it uses the RAM Legacy Stock Assessment database to fit a regression model to a series of characteristics of the catch time series and stock with stock-assessed $B/B_\mathrm{MSY}$ as the response. The model used in this paper is modified from the original --- it condenses the life-history categories into three categories to match the simulated dataset, removes the maximum catch predictor since the absolute catch in the simulated dataset is arbitrary, and does not implement the bias correction needed in @costello2012 since we do not derive estimates of median status across multiple stocks.

## Additional covariates

<!--TODO Kristin and Mark: It might be a good idea to add a figure that illustrates the spectrum of ensemble/superensembles that are applied here and what is included (e.g., non-linear relationships, etc.). This will help the reader understand the progression in complexity of the ensemble approaches I think.-->
Superensemble models allow us to incorporate additional covariates and potentially leverage interactions between these covariates and individual model predictions. Additional covariates could be, for example, life-history characteristics, information on exploitation patterns, or statistical properties of the data. For simplicity, and since many possible covariates, such as fishing effort dynamics, would not be known for many real-world data-limited scenarios, we added only one set of additional covariates: spectral properties of the catch time series. Spectral analysis decomposes a time series into the frequency domain and provides a means of describing the cyclical shape of the catch series that is independent of time series length (except in affecting precision) and absolute magnitude of catch. We fit spectral models to the scaled catch time series (catch divided by maximum catch) with the `spec.ar` function in \textsf{R} and recorded representative short- and long-term spectral densities at frequencies of 0.20 and 0.05, which correspond to 5- and 20-year cycles.

<!--*TODO add new supp. fig.?*-->

## Superensemble models

The individual models we seek to combine with superensembles provide time series of stock status ($B/B_\mathrm{MSY}$). Therefore, we can use superensembles to estimate multiple properties of these time series. Here, we focus on two properties: the mean and slope of $B/B_\mathrm{MSY}$ in the last 5 years. Together, these quantities address the recent state and trend of stock status, which are both of management and conservation interest [e.g. @mace2008]. To avoid undue influence of the end points of the time series on the calculated slope, we measured the slope as the Theil-Sen estimator of median slope [@theil1950].

We used the mean or slope of $B/B_\mathrm{MSY}$ as the response and the predictions from the individual models, and the spectral values, as predictors in our superensemble models. When modelling mean $B/B_\mathrm{MSY}$ --- a ratio bounded at zero --- we fit the superensemble models in log space and exponentiated the predictions. For the estimates of $B/B_\mathrm{MSY}$ slope, which are not bounded at zero, we fit superensemble models on the natural untransformed scale.

We compared an ensemble average and three superensembles of varying complexity: a linear model with two-way interactions, a random forest, and a boosted regression tree. We fit the linear model superensemble with all second-order interactions. Our two machine learning superensemble models, a random forest and a boosted regression tree, were based on regression trees. Regression trees sequentially determine what value of a predictor best splits the response data into two 'branches' based on a loss function [@breiman1984]. In random forests, a series of regression trees are built on a random subset of the data and random subset of the covariates of the model [@breiman2001]. In generalized boosted models (GBMs), each subsequent model is fit to the residuals from the previous model; data points that are fit poorly in a given model are given more weight in the next model [@elith2008]. Random forests and GBMs can provide strong predictive performance and fit highly non-linear relationships [@elith2008; @hastie2009]. We fit random forest models with the `randomForest` package [@liaw2002] for \textsf{R} with the default argument values. We fit boosted regression tree models with the `gbm` package [@ridgeway2015] for \textsf{R}. Based on initial tuning with the **caret** \textsf{R} package [@caret2008], we chose to fit GBMs with 2000 trees, an interaction depth of $6$, a learning rate (shrinkage parameter) of $0.01$, and all other arguments at their default values.

## Testing model performance

A critical component to predictive modelling is to evaluate the performance of a model on new data [@hastie2009]. We used repeated three-fold cross validation to test predictive performance: we randomly divided the dataset into three sets, built superensemble models on two-thirds of the data, and evaluated predictive performance on the remaining third. We repeated this across each of the three splits and then repeated the whole procedure 50 times to account for bias that may result from any one set of validation splits. In the simulated dataset, there were 10 replicates of each unique combination of simulation parameters that differed only in stochastic variability. Since the dynamics of these populations were often similar, we grouped these stocks in the cross-validation process into either the training or testing split. We also validated the models using the RAM Legacy Stock Assessment database. In this case, we used superensembles trained on the entire simulation dataset. However, since mPRM is built on the same stock-assessment database, we applied three-fold cross-validation to the RAM Legacy Stock Assessment data underlying the mPRM model so that the dataset that mPRM was trained with (for the individual model and superensemble) was separate from the dataset it was tested on.

Predictive performance can be evaluated with metrics that represent a variety of modelling goals. For continuous response variables such as the mean and slope of population status, performance metrics often measure some form of bias, precision, accuracy (a combination of bias and precision), or the ability to correctly rank or correlate across populations [e.g. @walther2005]. Here, we measure proportional error, defined as $(\hat{b} - b)/\abs{b}$, where $\hat{b}$ and $b$ represent estimated and 'true' (or stock-assessed) mean or slope of $B/B_\mathrm{MSY}$. We calculated median proportional error to measure bias, median absolute proportional error to measure accuracy, and Spearman's rank-order correlation between predicted and 'true' values to measure the ability to correctly rank populations. When cross-validating against the RAM Legacy Stock Assessment database, we treated the estimates from these data-rich stock assessments as known without error. Thus, any error in the stock-assessment estimates of the mean or slope of $B/B_\mathrm{MSY}$ also contributes to our estimates of prediction error for each of the four data-limited models and the ensembles. We measured the ability for each model to correctly place stocks into categories of above or below $B/B_\mathrm{MSY} = 0.5$ at any decision threshold by plotting receiver operating characteristic curves; a threshold of $B/B_\mathrm{MSY} = 0.5$ is used to classify a stock as overfished in Australia and the United States [@daff2007; @hilborn2010]. Code to reproduce our analysis and an \textsf{R} package to fit the data-limited assessment models are available at <https://github.com/...> (*Private while in review*).

<!-- TODO PE .... mean or slope of b/bmsy: Dan: logged or normal space? For the status regression where you are estimating log(B/Bmsy), wouldn't you need to correct for the retransformation bias in the estimating median proportional error later down? I could be missing something though in whether that applied once you've calculated proportional error-->

# Results

Applied to the simulated dataset of known stock status, the individual models had variable success at recovering the mean (status) and slope (trend) of $B/B_\mathrm{MSY}$ in the last five years. All models exhibited a high degree of scatter around the one-to-one line of perfect status prediction (Fig. \ref{hexagon}). In contrast to the known unimodal distribution of status, CMSY exhibited bimodal predictions (Fig. \ref{hexagon}a), but had the best rank-order correlation and accuracy scores (Fig. \ref{performance}a). COM-SIR and SSCOM both correctly identified a number of stocks with low status, but frequently predicted a high status when status was in fact low (Fig. \ref{hexagon}b, c). mPRM had relatively poor ability to predict status for the simulated dataset (Fig. \ref{hexagon}d). There was generally little correlation between true and predicted recent trend in status for any of the individual models (rank-order correlation = `r range_slope_corr_non_sscom`) with the exception of SSCOM (correlation = `r slope_corr_sscom`; Figs \ref{scatter-sim-slope}a--d).

Ensemble methods, and in particular the machine learning superensemble models (random forest and GBM), generally improved estimates of status and trend over any individual model (Fig. \ref{hexagon}e--h, Fig. \ref{scatter-sim-slope}e--h). Compared to the individual models, machine learning superensembles improved accuracy (median absolute proportional error) by `r mean_sim$mach_vs_ind_mare_fold`%, increased rank-order correlation from `r mean_sim$ind_corr_range` to `r mean_sim$mach_corr_range`, and reduced bias (median proportional error) from `r mean_sim$ind_mre_range` to `r mean_sim$mach_mre_range` (Fig. \ref{performance}a). These superensembles also generally had better ability to distinguish if simulated stocks were above or below $B/B_\mathrm{MSY} = 0.5$ (Fig. \ref{roc-sim}). Results were similar when predicting trend: compared to individual models, machine learning superensembles improved accuracy by `r slope_sim$mach_vs_ind_mare_fold`%, increased rank-order correlation from `r slope_sim$ind_corr_range` to `r slope_sim$mach_corr_range`, and reduced bias from `r slope_sim$ind_mre_range` to `r slope_sim$mach_mre_range` (Fig. \ref{performance-sim-slope}). The ensemble models that simply took a mean of the individual models ranked slightly behind the best individual model for estimating fish stock status (CMSY; Fig. \ref{performance}a) and had slightly lower correlation but higher accuracy than the best individual model at predicting the trends of status (SSCOM; Fig. \ref{performance-sim-slope}).

The superensemble models were able to improve the predictive performance by harnessing the best properties of individual models, the covariance between individual models, and interactions with other covariates. For example, SSCOM had strong predictive ability when it predicted low $B/B_\mathrm{MSY}$ (Fig. \ref{hexagon}c, Fig. \ref{partial-sim}c) and CMSY predictions were approximately linearly related to $B/B_\mathrm{MSY}$ within the low and high clusters of predictions (Fig. \ref{partial-sim}). Superensembles also exploited the covariance between individual model predictions. For instance, both the linear model and GBM ensemble suggest that if mPRM and SSCOM predict high status, the true status also tends to be high (Figs \ref{lm-coefs}, \ref{partial-2d-sim}l). The addition of spectral density covariates helped the superensemble models correctly predict higher status values (Fig. \ref{hexagon}g, h) due to the interactions between the spectral density predictors and the individual model predictors (Fig. \ref{partial-2d-sim}). Although the additional covariates improved ensemble fit, performance of the ensembles was only marginally degraded by removing these covariates (Fig. \ref{hexagon} vs. Fig. \ref{hexagon-sim-basic}).

When applied to the stock-assessment database, the superensemble models --- trained exclusively on the simulated dataset --- generally performed as well or better than the best individual models. The mean, random forest, and GBM ensembles outperformed the mPRM method which is trained directly on the RAM Legacy Stock Assessment database itself (Fig. \ref{performance}b, Fig. \ref{hexagon-ram}). Compared to the individual models, the machine learning superensembles increased accuracy by `r mean_ram$mach_vs_ind_mare_fold`%, improved correlation from `r mean_ram$ind_corr_range` to `r mean_ram$mach_corr_range`, and reduced bias from `r mean_ram$ind_mre_range` to `r mean_ram$mach_mre_range`.            

# Discussion

<!-- Dan:
Lastly, it's worth noting that this approach, as laid out here is for helping improve potentially conflicting estimates of the same metric, B/Bmsy. In many cases though a fishery is going to be basing its decisions on streams of multiple indicators, e.g trends in SPR and B/Bmsy. While the ensemble methods here can help refine estimates of individual metrics, it's still worth looking at what alternative data streams are telling you if possible. E.g. if a purely catch based B/Bmsy estimate is high and has a positive slope, but mean lengths and SPR are decreasing, managers will still need to think about how to deal with these conflicting signals. So, this ensemble method, as I interpret it, is helpful for resolving conflicts within metric, but doesn't necesarily resolve conflicts across metrics (if I'm understanding these methods right)
-->

Ensemble methods provide a useful approach to situations where conservation and environmental resource management decisions must be made on the basis of multiple, potentially contrasting estimates of status --- a situation common to many settings beyond fisheries. Compared to individual models of fish population status, ensemble methods were consistently the best or among the best across three performance dimensions (accuracy, bias, and rank-order correlation), two response variables (status and trend), two datasets (simulated and global fisheries), and multiple ensemble methods (from a simple average to machine learning superensembles). Our results suggest choosing a superensemble model that allows for non-parametric relationships, such as machine learning methods. These methods provide added insight into individual model behavior and generally performed the best; however, even a simple average of predictions across multiple models may provide a more useful measure of the desired parameter than a single model in many cases.

Certain conditions will make some ensemble models more effective than others. First, ensembles will be most effective when they are comprised of diverse individual models that choose different structural model forms, explore contrasting but plausible ranges of parameter values, or make uncorrelated errors [@ali1996; @dietterich2000; @tebaldi2007]. Such individual models would be expected to perform well in different conditions and an ensemble model can exploit the best predictive performance of each. Second, ensemble models will be most effective when they are not overfit to the training dataset. Cross-validation testing [@caruana2004; @hastie2009] and methods that are robust to overfitting such as random forests [@breiman2001], may help avoid overfitting ensemble models. We note that our simplest ensemble model, an average of individual model predictions, performed approximately as well as complex machine learning models when we trained our superensembles on the simulation dataset and tested them on a separate 'real' dataset (i.e. the RAM Legacy Stock Assessment database, Fig. \ref{performance}b). Third, ensemble models will be most effective when they are trained on data that are representative of the dataset of interest [@knutti2009; @weigel2010]. Cross-validation within a training dataset will provide an optimistically biased impression of predictive performance if the training dataset fundamentally differs from the dataset of interest [@hastie2009].

Multi-model inference in the form of coefficient averaging weighted by information theoretics such as the Akaike Information Criterion (AIC) is a common analytical approach in ecology [@burnham2002; @johnson2004; @grueber2011]. The ensemble methods described in this paper share similarities with coefficient averaging but differ in other important ways. Ensemble methods and coefficient averaging share the long-held notion that multiple working hypotheses can contribute useful information for inference [@chamberlin1890]. A fundamental difference is that coefficient averaging focuses on averaging *coefficients* whereas ensembles instead average *predictions*. Thus, while the outcome will be the same in some cases, ensembles provide a more general purpose tool: they do not require information theoretics and they can combine different types of models (e.g. parametric and non-parametric models or frequentist and Bayesian predictions). Furthermore, superensembles extend these benefits by allowing model predictions to be combined via non-linear functions that are tuned to known data.

A strength of superensembles is that they can be tailored to predict specific response variables. For example, we built separate superensemble models of mean $B/B_\mathrm{MSY}$ and the slope of $B/B_\mathrm{MSY}$. The same set of model weights or non-linear relationships need not hold across different response variables. For instance, SSCOM contributed little to the GBM superensemble estimate of status at higher levels of predicted $B/B_\mathrm{MSY}$ (Fig. \ref{partial-sim}), but contributed strongly to estimates of trend (Fig. \ref{partial-sim-slope}). Formally, fitting superensemble models to specific quantities of interest provides an additional calibration step [@rykiel1996]. Individual models are typically calibrated (i.e. parameters are estimated) based on a single response variable; however, this model might then provide a biased or inaccurate estimate of other quantities of interest. Ensemble methods provide a final calibration step to a quantity of interest. This ensemble calibration could further include a loss function tailored to
 the goals of the model, say placing greater weight on accuracy at low status levels than higher values. Conversely, because superensembles are tailored to a specific response and loss function, superensembles force a modeller to choose an operational purpose for their model up front [*sensu* @dickey-collas2014].

As @box1987 noted, all models are wrong, but some may still be useful. The ensemble methods we investigated attempt to piece together the useful parts of candidate models to build a model with improved performance. Instead of viewing the superensemble as a black box, we think considerable mechanistic understanding can be gained by studying its structure. For example, when SSCOM estimates very low status this is likely the case, conversely when COMSIR estimates low status, the true status is more likely to be high (Fig. \ref{partial-sim}). These models have two main differences: (1) the form of effort dynamics and (2) the allowance for both measurement and process error in SSCOM, whereas the implemented COMSIR admits measurement error only. Were the methods to differ only in effort dynamics, the results point towards a more suitable representation of effort dynamics at low biomasses in SSCOM. We think that such investigation of the parameters of a superensemble may lead to improvement in the mechanisms assumed in the individual models.

Combining predictions from multiple models via superensemble methods is also broadly useful in other subfields of ecology and fisheries science. Predictions about extinction risk are widely used at the national (e.g. the US Endangered Species Act and the Canadian Species at Risk Act) and international [e.g. the IUCN Red List, @iucn2015] levels. These risk assessments generally involve fitting regression models to outcomes for individual species along with predictors of extinction risk [e.g. @anderson2011a; @pinsky2011], or population dynamics models to data for individual species [e.g. @dfo2010]. Both types of models are prone to error caused by model-misspecification and therefore results are sensitive to decisions about model structure [@brooks2015]. Although there are options to account for potential model-misspecification in determination of species risk [e.g. coefficient averaging, @burnham2002; generalized modeling, @yeakel2011; or semi-parametric methods, @thorson2014], ensemble methods are a relatively simple way to combine predictions in a transparent manner. Beyond estimates of status and trend, ensemble methods could be used, for example, to increase the robustness of spatial predictions when designing networks of protected areas [@rassweiler2014] or to forecast potential spatial shifts in species distribution given climate impacts [@harsch2014].

<!-- TODO put back when space: or to evaluate the effect of conservation management actions when experimental or observational data are limited [@walsh2012].-->

\noindent
**Acknowledgements**: We thank members of Phase I of the working group 'Developing new approaches to global stock status assessment and fishery production potential of the seas' who
contributed to developing the data-limited methods and simulations used in our
analysis.
We thank E. Jardim, F. Scott, and J.A. Hutchings for helpful comments during the development of this project, and R.D. Methot for comments on an earlier version of the manuscript.
We thank the Gordon and Betty Moore Foundation for funding the working group
'Applying data-limited stock status models and developing management guidance for unassessed fish stocks'.


<!--Funding...-->

<!--# Citation notes-->

<!--
- @breiner2015 ensemble models of species distribution models for rare species

- @jones2015 ensemble models of species distribution models - globally for
  marine biodiversity

- [@greene2006] example similar to ours but with climate models (Bayesian
  multilevel ensemble)

- [@kell2007] FLR

- multiple learners book [@alpaydin2010]

- [@caruana2004] nice paper on ensemble models; prob of overfitting increases
  with more models, bagging important

- [@tebaldi2007] key paper: review of 'multi-model ensembles for climate projections'

- famous ensemble is DEMETER: Development of a European Multi-model Ensemble
  System for Seasonal to Interannual Prediction @hagedorn2005 is a good
  reference

- Examples of where ensemble models are shown to be better than any one:
  @thomson2006 (public health - malaria), @cantelaube2005 (agriculture - crop
  yield) (citations taken from @tebaldi2007)

- simple averages are used very commonly in climate science - e.g. IPCC 2001

- @tebaldi2007: weighting obviously makes sense, but how do we define a
  performance metric that is based on past observations that is relevant to the
  future? 

- @tebaldi2007: model independence important

- for climate, weighted averages perform better than simple averages [@min2006],
  but are those same weights applicable to the future (or in our case other
  fisheries)?

Why do ensemble methods work?

- error cancellation is one but not the only reason for superiority of ensemble
  models [@hagedorn2005]

- ensemble model may be only marginally better than the best single model in
  any given case, but we don't usually know which is the best single model
  [@hagedorn2005]

- ensemble models useful for regional climate models too; as an example,
  @pierce2009 use 42 metrics to characterize model performance in regional
  climate model ensembles; found that ensemble models were superior to any one
  model --- especially when considering multiple metrics

- @knutti2009: key review paper on motivation and challenges of combining
  climate projection models; performance on testing / current data may only
  weakly relate to future / other datasets

- @murphy2004: Nature paper on ensembles of climate model simulations; weighted
  average better performance than unweighted average

- @dietterich2000: highly cited book chapter "Ensemble Methods in Machine
  Learning"; ensembles are often much more accurate than the "individual
  classifiers that make them up"; but components must be diverse and accurate

- @dietterich2000: a main justification for ensembles: they are
  representational --- no one model usually can contain all hypothetical
  functional forms, but many separate models can cover more hypotheses

- strong paper showing that ensemble models are most accurate when the various
  individual models make errors in uncorrelated ways [@ali1996]

- without substantial training-testing data, appropriate model weights can be
  very hard to deduce and can cause more harm than good (compared to equal
  weighting) [@weigel2010] (they give the example of seasonal forecasting where
  a ton of hind cast testing can be done, vs. long-term climate) (asymmetrical
  loss function)

- [@weigel2010] optimal weights are always as good or better than equal
  weights, but if you get the weights wrong, you can be better off just using
  equal weights; but averaging was almost always better than any one model

- @rykiel1996 "Testing ecological models: the meaning of validation" (see for
  performance criteria, theory on model testing and assessment)

model averaging AIC:
f Chamberlin 1890).  multiple woring hypotheses
  (e.g., Johnson and Omland 2004, Hobbs and Hilborn 2006, Burnham et al. 2011,
  52 Grueber et al. 2011). 
-->

