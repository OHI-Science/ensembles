```{r, echo=FALSE}
# Values to use in the paper:
load("values.rda") # generated by `../analysis/9-output-values.R`

# Includes: mean_sim, mean_ram, slope_sim, auc_sim, ram_stocks_n

# mean_sim, mean_ram, and slope_sim have the following list elements:
# > names(mean_sim)
# "mach_vs_ind_mare_fold" "ind_corr_range" "mach_corr_range" "ind_mare_range"  
# "mach_mare_range" "ensemble_mre_range" "ind_mre_range"
 
# auc_sim has the following list elements:
# > names(auc_sim)
# "mach_range" "ind_range" 
```

\noindent
Sean C. Anderson^1\*^, 
Jamie Afflerbach^2^, 
Mark Dickey-Collas^3^, 
Olaf P. Jensen^4^, 
Kristin M. Kleisner^5^, 
Catherine Longo^2^, 
C\'{o}il\'{i}n Minto^6^, 
Giacomo Chato Osio^7^, 
Dan Ovando^8^, 
Andrew A. Rosenberg^9^, 
Elizabeth R. Selig^10^, 
James T. Thorson^11^,
Jessica C. Walsh^1^,
Andrew B. Cooper^1^
*(order to be determined and others may be added)*

\bigskip

\noindent
^1^School of Resource and Environmental Management,
Simon Fraser University, Burnaby, BC, V5A 1S6, Canada

\noindent
^2^National Center for Ecological Analysis and Synthesis

\noindent
^3^International Council for the Exploration of the Sea

\noindent
^5^NOAA/NMFS/NEFSC, Ecosystem Assessment Program, 
Woods Hole, MA, 02543, USA

\noindent
^6^Galway-Mayo Institute of Technology, 
Marine and Freshwater Research Centre, Galway, 00000, Ireland

\noindent
^7^EC JRC, IPSC, MAU, Italy

\noindent
^8^University of California Santa Barbara, 
Bren School of Environmental Science and Management, 
Santa Barbara, CA, 93106-5131, USA

\noindent
^9^Union of Concerned Scientists, Cambridge, MA, USA

\noindent
^10^Conservation International, Arlington, VA, USA

\noindent
^11^National Marine Fisheries Service, 
National Oceanic and Atmospheric Administration, 
Fisheries Resource Assessment and Monitoring Division, 
Northwest Fisheries Science Center, Seattle, WA, 98112, USA

\noindent
^\*^Corresponding author: Sean C. Anderson,
School of Resource and Environmental Management,
Simon Fraser University,
Burnaby BC, V5A 1S6;
E-mail: sean_anderson@sfu.ca

\clearpage

# Abstract

What can we do when multiple ecological models give different impressions of population status? Any single model is unlikely to perform best in all scenarios. 
However, ensemble methods from the fields climate science and machine learning provide a powerful tool to leverage information across multiple ecological models. 
Here we evaluate the potential for ensemble methods to improve estimates of fish population status. 
We fit four data-limited models of population and exploitation dynamics and combine their estimates of the mean and slope of biomass at maximum sustainable yield (\bbmsy\\) with four ensemble models: a simple average, a linear model, and two machine-learning methods---random forests and boosted regression trees. 
We build our ensembles on a simulated dataset of 5760 fish stocks and test our models with cross-validation and a global database of `r ram_stocks_n` stock assessments. 
We find that ensemble models substantially improve estimates of population status and trajectory. 
Machine-learning ensembles performed the best: accuracy improved `r mean_sim$mach_vs_ind_mare_fold`%, correlation improved from `r mean_sim$ind_corr_range` to `r mean_sim$mach_corr_range`, and bias (median proportional error) declined from `r mean_sim$ind_mre_range` to `r mean_sim$mach_mre_range`. 
We found similar improvements when predicting the slope of status. 
When the simulation-trained ensembles were applied to global fish stocks, the best ensembles still improved all dimensions of performance over the next-best individual model. 
Ensemble models have potential to improve estimates of population status and trajectory across many taxa beyond fish. 
However, ensemble methods must be carefully tested, formed from a diverse set of accurate models, and built on a dataset sufficiently representative of the populations they are applied to.

# Introduction

We often have multiple models of ecological systems. For example, models might
incorporate different data types, assume alternate population dynamics, or make
contrasting assumptions about the starting state of a system. These models may
suggest different conclusions about population status and often times we do not
know which model is best. How can we reconcile multiple models to make robust
management decisions about ecological resources?

Ensemble models, which combine the output from multiple models, are one
potential solution to this problem. A simple ensemble model might take the
average of multiple model outputs, perhaps weighted by some goodness of fit. A
complex ensemble model might also take into account the interactions between
model outputs and additional covariates. Ensemble models are popular in other
fields including public health [e.g. @thomson2006], agriculture [e.g.
@cantelaube2005], climate science [e.g. @murphy2004; @tebaldi2007;
@pierce2009], and machine learning [e.g. @dietterich2000]. We think they have
tremendous opportunity in ecological resource management.

In fisheries science, a common problem is estimating the status of an exploited
fish population. For the majority of fish stocks, we have limited biological
information and stock status isn't known [@fao2014]. In recent years, a number
of methods have been proposed to derive population status based on the limited
information that is frequently available: fisheries catch time series and basic
knowledge about the productivity of the species. Recently, @rosenberg2014
investigated the ability for these models to estimate population status through
a large-scale simulation model. A key finding was that these models frequently
disagree on population status and any one model has relatively poor predictive
ability on average (e.g. Fig. \ref{fig:motivate}).

<!--
Exploit interactions between individual models and relationships between other
conditions of the data and the performance of the individual models.
Fitted to populations of known or assumed status and this ensemble model is
used to extrapolate to populations of interest.
-->

Here, we develop ensemble models to estimate the population status of exploited
fish populations. We explore a variety of ensemble approaches applied to both
simulated and real-world fish stocks and compare predictive performance. In
developing these models we highlight the types of problems one encounters when
forming and evaluating ensemble models. We find that ensembles generally
provide improved predictive performance than any single model---especially when
considered across multiple dimensions of predictive ability. Furthermore, the
output from ensemble models can provide better mechanistic understanding of how
individual models perform.

# Methods

We tested the ability of ensemble models to improve estimates of population
status when applied to both a large simulated dataset and a global database of
assessed stock status. Here we describe the datasets, individual models of
population status, and ensemble models to combine those estimates. We then
describe how we evaluated the ability of the various models to estimate
population status.

## Datasets

<!--TODO: github flr repo not available--> 

We first developed and tested ensemble models with a fully factorial simulated
dataset [@rosenberg2014]. In summary, the simulation models included three life
histories: small pelagic, demersal, large pelagic; three levels of how depleted
biomass was at the start of the dataset compared to carrying capacity: 100%,
70%, and 40% of carrying capacity; and four harvest dynamics: a constant
harvest rate, a harvest rate that is coupled with biomass to mimic an
open-access single-species fishery, a scenario where harvest rate increases to
a fixed level, a 'roller-coaster' scenario where the harvest rate increases,
maintains a steady level and decreases to a lower steady level. Process noise
was introduced to the models through two levels of multiplicative recruitment
variability: $N(0, 0.2)$ and $N(0, 0.6)$, and that process noise was either
uncorrelated or had a first-order autoregressive correlation of 0.6. The
simulation included a scenario without observation error and with
multiplicative observation error around catch at $N(0, 0.2)$. 

@rosenberg2014 ran ten iterations for each combination of factors adding
stochastic draws of recruitment and catch-recording variability each time to
generate a total of 5760 stocks. The simulation models were built in the
package FLR [@kell2007] for the statistical software \textsf{R} [@r2015].
@rosenberg2014 provide a full description of the simulation model and the code
to generate the simulations is available at <https://github.com/flr/StockSims>
(TODO: not there). 

We also tested our ensemble models on the RAM Legacy Stock Assessment Database
[@ricard2012]. Our analysis of the stock-assessment database was based on
version XX downloaded on XX. After removing stocks for which the models
described below did not converge, this database included `r ram_stocks_n` from
XX countries across XX taxonomic orders.

## Individual models of population status

We fit four individual data-limited models to estimate \bbmsy\\. Three of the
models are mechanistic and based generally on @schaefer1954 biomass
dynamics of the form:

$$\hat{B}_{t+1} = B_t + r B_t \left(1 - B_t / B_0 \right) - C_t,$$

\noindent where $\hat{B}_{t+1}$ represents predicted biomass at time $t$ plus
one year, $B_t$ represents biomass at time $t$, $r$ represents the intrinsic
population growth rate, $B_0$ represents unfished biomass or carrying capacity,
and $C$ represents catch. @rosenberg2014 provide a full summary of these four
methods and code to fit all the models is available in an accompanying package
**datalimited** for the statistical software \textsf{R}. In summary:

- *COM-SIR* (catch-only-model with sample-importance-resampling) is a coupled
harvest-dynamics model developed by @vasconcellos2005. Biomass is assumed to
follow a Schaefer model and harvest dynamics are assumed to follow a logistic
model. The model is fit with a sample-importance-sampling algorithm developed
in @rosenberg2014.

- *CMSY* (catch-MSY) implements a stock-reduction analysis with Schaefer biomass
dynamics [@martell2013]. It requires a prior distribution for $r$ and assigns
a prior to the relative proportion of biomass at the end compared to unfished
biomass (depletion) based on the percentage of maximum catch at the end of the
time series.

- *SSCOM* (state-space catch-only-model) TODO: Jim -- can you give a ~2 sentence summary?

- *mPRM* (modified panel regression model) is a modified version of the
panel-regression model used in @costello2012. Unlike the other models
considered, mPRM is empirical and not mechanistic --- it fits a linear model to
a series of characteristics of the catch time series and stock with \bbmsy\\ as
the response. The model used in this paper and @rosenberg2014 is modified from
the original in that it simplifies the life-history categories to be applicable
to simulated data.
<!--TODO: what are we doing with the RAM stocks again?-->

<!--TODO: build in fig:didactic -->

## Additional covariates

Ensemble methods allow us to incorporate additional covariates into our models
and potentially leverage interactions of how these covariates combine with the
main individual model estimates to estimate population status. Examples might
be life-history characteristics, additional information on exploitation
patterns, or additional statistical properties of the data going into the
individual models. For simplicity, and to allow us to apply models developed
with the simulated dataset to the real-world dataset, we added only one set of
additional covariates: spectral properties of the catch time series itself.
Spectral analysis decomposes a time series into the frequency domain and
provides a useful means of describing the statistical properties of the catch
series that is independent of time series length and magnitude of catch. We fit
spectral models to the scaled catch time series (divided by maximum catch) with
the `spec.ar` function in \textsf{R} and recorded representative short- and
long-term spectral densities of 0.05 and 0.2, which correspond to 20- and
5-year cycles.

## Ensemble models

The individual models we seek to combine with ensemble models provide time
series of \bbmsy\\ ($\hat{b}_\mathrm{CMSY}$, $\hat{b}_\mathrm{COM-SIR}$, etc.).
Therefore, we can use ensemble models to estimate multiple properties of these
status time series. Here, we focus on two values: the mean and slope of
\bbmsy\\ in the last 5 years. Below, we refer to these quantities collectively
as $\hat{b}_\mathrm{ensemble}$. Together, these quantities address the current
state and trajectory of status, both of which may be of management and
conservation interest. To avoid undue influence of the end points of the time
series on the calculated slope, we measured the slope as the Theil-Sen
estimator of median slope [@theil1950].

An ensemble model can combine models through anything from a simple average to
a complex non-linear model. Here we choose four ensemble models to compare: an
average, a linear model with two-way interactions, a random forest, and boosted
regression tree. The general form of our models is as follows:

$$
\hat{\theta} = 
\mathrm{argmin}_\theta \left( L(b \, | \, \hat{b}_{\mathrm{ensemble}}) \right)
$$
$$
\hat{b}_\mathrm{ensemble} = 
f \left( \hat{b}_\mathrm{CMSY}, \, \hat{b}_\mathrm{COM-SIR}, \,
\hat{b}_\mathrm{SSCOM}, \, \hat{b}_\mathrm{mPRM}, \,
S(0.2), \, S(0.5), \, | \, \theta \right)
$$

\noindent where $L$ is a generalized loss function, $f$ is some generalized
regression function, $\hat{b}_\mathrm{CMSY}$ is the prediction from the CMSY
model etc. (Table \ref{tab:predictors}), $S(n)$ represents the spectral density
at frequency $n$ of the scaled catch series, $b$ is the true value the model is
fitted to, and $\theta$ is the set of parameters for the regression function.
For all ensemble models of mean \bbmsy\\, which is bounded at zero, we fit the
models in log space and exponentiated the predictions to derive a median
estimate of \bbmsy\\. For the estimates of \bbmsy\\ slope we fit ensemble
models on the natural untransformed scale.

We calculated the average ensemble models as the geometric mean of the
individual model median \bbmsy\\ estimates and the mean of the individual model
\bbmsy\\ slopes. We fit the linear model ensemble with second-order
interactions. We fit two machine learning ensemble models based on regression
trees. Regression trees sequentially determine what value of a predictor best
splits the response data into two 'branches' based on a specified loss
function. In random forests, a series of regression trees are built on a random
subset of the data and random subset of the covariates of the model (REF). In
generalized boosted models (GBMs), each subsequent model is fit to the
residuals from the previous model; data points that are fit poorly in a given
model are given more weight in the next model [@elith2008]. Random forests and
GBMs can provide strong predictive performance and fit highly non-linear
relationships (REFs). We fit random forest models with the **randomForest**
package [@liaw2002] for \textsf{R} with the default argument values. We fit
boosted regression tree models with the **GBM** package [@ridgeway2015] for
\textsf{R}. Based on cross-validation with the **caret** package (Supporting
Materials, Fig. SX), we fit GBMs with 2000 trees, an interaction depth of $6$,
a learning rate (shrinkage parameter) of $0.01$, and all other arguments at
their default values.                                                

## Testing model performance

A critical component to any predictive modelling exercise is to evaluate the
performance of a model new data [@hastie2009]. We used repeated three-fold
cross validation to test predictive performance: we randomly divided the
dataset into three sets, built ensemble models on two-thirds of the data, and
evaluated predictive performance on the remaining third of the data. We
repeated this across each of the three splits and then repeated the whole
procedure XX times to account for bias that may result from any one set of
validation splits. Since the dynamics of simulated populations differing only
in the magnitude of observation error around catch, magnitude of recruitment
variability, or stochastic draw of these values are likely similar, we grouped
these characteristics together in the cross-validation process---we never
trained and tested on models differing only in these characteristics.

Predictive performance can be evaluated with metrics that represent a variety
of modelling goals. For continuous response variables such as the mean and
slope of population status, performance metrics often measure some form of bias,
precision, accuracy (a combination of bias and precision), or the ability to
correctly rank or correlate across populations. Here, we measure proportional
error (PE; also called 'relative error') defined as $(\hat{b} - b)/b$. We
summarize median proportional error to measure bias, median absolute
proportional error to measure accuracy, and Spearman's rank-order correlation
to measure the ability to correctly rank populations.

# Results

CMSY has bimodality (Fig. \ref{fig:hexagon}a)

Applied to the simulated data (Fig. \ref{fig:hexagon}e--h, Fig. \ref{fig:performance}a)...

machine learning models and linear models outperformed all individual models in terms of accuracy (MAPE) and Spearman's rank-order correlation across stocks

ensembles generally had lower bias (MPE) than any individual model (Fig. \ref{fig:performance})

ensemble models generally had better ability to distinguish if simulated stocks were above or below $B/B_\mathrm{MSY}$ = 1$ (Fig. \ref{fig:roc-sim})

ensembles also did better at estimating the slope of \bbmsy\\ in the last five years (Figs. \ref{fig:scatter-sim-slope}, \ref{fig:performance-sim-slope}).

for slope: again, machine learning ensembles best by rank-order correlation and accuracy and had lowest bias (numbers) (Fig. \ref{fig:performance-sim-slope})

Although the additional spectral density covariates improved ensemble fit, performance of the ensembles was only marginally degraded by removing these covariates (Fig. ~\ref{fig:hexagon} vs. Fig. \ref{fig:hexagon-sim-basic})

For example, the linear model ensemble suggests that if both mPRM and SSCOM predict high \bbmsy\\, the true \bbmsy\\ is also high (Fig. \ref{fig:lm-coefs})

The GBM model, also indicates a similar relationship, but suggests a non-linear relationship between these individual models in predicting \bbmsy\\ (Fig. \ref{fig:partial-2d-sim}XX)

Partial dependence plots illustrate the non-linear relationship between the
various individual model predictions and the ensemble predictions (Fig.
\ref{fig:partial-sim}). These integrate across other predictor values...

Ensemble models can exploit the best performance characteristics of individual models. 
For example, if SSCOM predicts a low \bbmsy\\ (less than about 0.X), true \bbmsy\\ is probably low (Fig. \ref{fig:hexagon}c).
But, SSCOM provides less predictive ability on its own when it predicts higher \bbmsy\\. The machine learning ensemble models can exploit this (Fig. \ref{fig:partial-sim}XX). On its own, CMSY contributes the most to predicting \bbmsy\\ when it makes predictions of \bbmsy\\ over one.

Ensemble models trained on the simulated dataset, outperformed any individual model across all performance dimensions when applied to a completely new dataset: the RAM Legacy Stock Assessment database. They even outperform the mPRM method which is trained on the stock assessment database itself (Fig. \ref{fig:hexagon-ram})

Ensembles take advantage of not just the best aspects of individual models, but also the interactions between multiple individual models or the interaction between individual models and other covariates. For example, CMSY becomes more informative about mean \bbmsy\\ when combined with the spectral density measures (Fig. \ref{fig:partial-2d-sim}XX, XX).

Across simulated data mean \bbmsy\\, stock assessment database \bbmsy\\ and
simulated data slope, the ensembles consistently had the best, or among the best levels of accuracy, rank order correlation, and level of bias (Figs \ref{fig:performance}, \ref{fig:performance-sim-slope}).

Some individual models performed nearly as well as the enembles in one or two performance dimensions: e.g. SSCOM does nearly as well as the ensembles at predicting the slope of \bbmsy\\ (Fig. \ref{fig:performance-sim-slope}). But no individual method is consistently nearly as good as the ensembles. SSCOM does more poorly than the ensembles and other individual models in terms of accuracy, rank-order correlation, and bias at estimating the mean \bbmsy\\ for both the simulated and stock-assessment datasets (Fig. \ref{fig:performance).

# Discussion

# Acknowledgements

<!--Funding...-->

<!--# Citation notes-->

<!--
- @breiner2015 ensemble models of species distribution models for rare species

- @jones2015 ensemble models of species distribution models - globally for
  marine biodiversity

- [@greene2006] example similar to ours but with climate models (Bayesian
  multilevel ensemble)

- [@kell2007] FLR

- multiple learners book [@alpaydin2010]

- [@caruana2004] nice paper on ensemble models; prob of overfitting increases
  with more models, bagging important

- [@tebaldi2007] key paper: review of 'multi-model ensembles for climate projections'

- famous ensemble is DEMETER: Development of a European Multi-model Ensemble
  System for Seasonal to Interannual Prediction @hagedorn2005 is a good
  reference

- Examples of where ensemble models are shown to be better than any one:
  @thomson2006 (public health - malaria), @cantelaube2005 (agriculture - crop
  yield) (citations taken from @tebaldi2007)

- simple averages are used very commonly in climate science - e.g. IPCC 2001

- @tebaldi2007: weighting obviously makes sense, but how do we define a
  performance metric that is based on past observations that is relevant to the
  future? 

- @tebaldi2007: model independence important

- for climate, weighted averages perform better than simple averages [@min2006],
  but are those same weights applicable to the future (or in our case other
  fisheries)?

- error cancellation is one but not the only reason for superiority of ensemble
  models [@hagedorn2005]

- ensemble model may be only marginally better than the best single model in
  any given case, but we don't usually know which is the best single model
  [@hagedorn2005]

- ensemble models useful for regional climate models too; as an example,
  @pierce2009 use 42 metrics to characterize model performance in regional
  climate model ensembles; found that ensemble models were superior to any one
  model --- especially when considering multiple metrics

- @knutti2009: key review paper on motivation and challenges of combining
  climate projection models; performance on testing / current data may only
  weakly relate to future / other datasets

- @murphy2004: Nature paper on ensembles of climate model simulations; weighted
  average better performance than unweighted average

- @dietterich2000: highly cited book chapter "Ensemble Methods in Machine
  Learning"; ensembles are often much more accurate than the "individual
  classifiers that make them up"; but components must be diverse and accurate

- @dietterich2000: a main justification for ensembles: they are
  representational --- no one model usually can contain all hypothetical
  functional forms, but many separate models can cover more hypotheses

- strong paper showing that ensemble models are most accurate when the various
  individual models make errors in uncorrelated ways [@ali1996]

- without substantial training-testing data, appropriate model weights can be
  very hard to deduce and can cause more harm than good (compared to equal
  weighting) [@weigel2010] (they give the example of seasonal forecasting where
  a ton of hind cast testing can be done, vs. long-term climate) (asymmetrical
  loss function)

- [@weigel2010] optimal weights are always as good or better than equal
  weights, but if you get the weights wrong, you can be better off just using
  equal weights; but averaging was almost always better than any one model

- @rykiel1996 "Testing ecological models: the meaning of validation" (see for
  performance criteria, theory on model testing and assessment)
-->
