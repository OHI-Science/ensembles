```{r, echo=FALSE}
# Values to use in the paper:
load("values.rda") # generated by `../analysis/9-output-values.R`

# Includes: mean_sim, mean_ram, slope_sim, auc_sim, ram_stocks_n

# mean_sim, mean_ram, and slope_sim have the following list elements:
# > names(mean_sim)
# "mach_vs_ind_mare_fold" "ind_corr_range" "mach_corr_range" "ind_mare_range"  
# "mach_mare_range" "ensemble_mre_range" "ind_mre_range"
 
# auc_sim has the following list elements:
# > names(auc_sim)
# "mach_range" "ind_range" 
```

\noindent
Sean C. Anderson^1\*^, 
Jamie Afflerbach^2^, 
Mark Dickey-Collas^3^, 
Olaf P. Jensen^4^, 
Kristin M. Kleisner^5^, 
Catherine Longo^2^, 
C\'{o}il\'{i}n Minto^6^, 
Giacomo Chato Osio^7^, 
Dan Ovando^8^, 
Andrew A. Rosenberg^9^, 
Elizabeth R. Selig^10^, 
James T. Thorson^11^,
Jessica C. Walsh^1^,
Andrew B. Cooper^1^
*(order to be determined and others may be added)*

\bigskip

\noindent
^1^School of Resource and Environmental Management,
Simon Fraser University, Burnaby, BC, V5A 1S6, Canada

\noindent
^2^National Center for Ecological Analysis and Synthesis

\noindent
^3^International Council for the Exploration of the Sea

\noindent
^5^NOAA/NMFS/NEFSC, Ecosystem Assessment Program, 
Woods Hole, MA, 02543, USA

\noindent
^6^Galway-Mayo Institute of Technology, 
Marine and Freshwater Research Centre, Galway, 00000, Ireland

\noindent
^7^EC JRC, IPSC, MAU, Italy

\noindent
^8^University of California Santa Barbara, 
Bren School of Environmental Science and Management, 
Santa Barbara, CA, 93106-5131, USA

\noindent
^9^Union of Concerned Scientists, Cambridge, MA, USA

\noindent
^10^Conservation International, Arlington, VA, USA

\noindent
^11^National Marine Fisheries Service, 
National Oceanic and Atmospheric Administration, 
Fisheries Resource Assessment and Monitoring Division, 
Northwest Fisheries Science Center, Seattle, WA, 98112, USA

\noindent
^\*^Corresponding author: Sean C. Anderson,
School of Resource and Environmental Management,
Simon Fraser University,
Burnaby BC, V5A 1S6;
E-mail: sean_anderson@sfu.ca

\clearpage

# Abstract

What can we do when multiple ecological models suggest different conclusions about population status? Any single model is unlikely to perform best under all scenarios. Ensemble methods, commonly used in fields such as climate science, provide a powerful tool to leverage information across multiple ecological models. Here we evaluate the potential for ensemble methods to improve estimates of fish population status and trajectory. We fit four data-limited models of population and exploitation dynamics and combine their estimates of the mean and slope of biomass at maximum sustainable yield (\bbmsy\\) with four ensemble models: a simple average, a linear model, and two machine-learning methods---random forests and boosted regression trees. We build our ensembles on a simulated dataset of 5760 fish stocks and test our models with cross-validation and a global database of `r ram_stocks_n` stock assessments. We find that ensemble models substantially improve estimates of population status and trajectory. Machine-learning ensembles performed the best: accuracy improved `r mean_sim$mach_vs_ind_mare_fold`%, correlation improved from `r mean_sim$ind_corr_range` to `r mean_sim$mach_corr_range`, and bias (median proportional error) declined from `r mean_sim$ind_mre_range` to `r mean_sim$mach_mre_range`. We found similar improvements when predicting the slope of status. When the simulation-trained ensembles were applied to global fish stocks, the best ensembles still improved all dimensions of performance over the next-best individual model. Ensemble models have potential to improve estimates of population status and trajectory across many taxa beyond fish. 
However, ensemble methods must be carefully tested, formed from a diverse set of accurate models, and built on a dataset sufficiently representative of the populations they are applied to.

# Introduction

There are often multiple models we can apply to an ecological system to assess population status and trajectory. Models might incorporate different data types, assume alternate population dynamics, or make contrasting assumptions about the starting state of a system. These models may suggest different conclusions about population status and trajectory and we often do not know which model is best. How can we reconcile multiple models to make robust management decisions about ecological resources? 

Ensemble methods, which combine the output from multiple models as predictors in a new 'ensemble' model, are one solution to this problem. A simple ensemble model might take the average of multiple model outputs, perhaps weighted by some goodness of fit. A complex ensemble model might also take into account non-linear interactions between model outputs and additional covariates. Ensemble models are popular in other fields including public health [e.g. @thomson2006], agriculture [e.g. @cantelaube2005], climate science [e.g. @murphy2004; @tebaldi2007; @pierce2009], and machine learning [e.g. @dietterich2000]. We think they have tremendous opportunity in ecological resource management. 

In fisheries science, a common task is estimating the status and trajectory of an exploited fish population. For the majority of fish stocks, we have limited biological information and stock status isn't known [@fao2014]. In recent years, a number of methods have been proposed to derive population status based on the limited information that is broadly available: fisheries catch time series and basic knowledge about the productivity of the species. Recently, @rosenberg2014 investigated the ability for these models to estimate population status through a large-scale simulation model. A key finding was that these models frequently disagree on population status and any one model has relatively poor predictive ability on average (e.g. Fig. \ref{motivate}).

<!--
Exploit interactions between individual models and relationships between other conditions of the data and the performance of the individual models. Fitted to populations of known or assumed status and this ensemble model is used to extrapolate to populations of interest.
-->

Here, we develop ensemble models to estimate the population status and trajectory of exploited fish populations. We explore a variety of ensemble approaches applied to both simulated and real-world fish stocks and compare predictive performance. In developing these models we highlight the types of challenges one encounters when forming and evaluating ensemble models. We find that ensembles generally provide improved predictive performance than any single model---especially when considered across multiple dimensions of predictive ability and across multiple datasets. Furthermore, the output from ensemble models can provide better mechanistic understanding of how individual models of status and trajectory perform.

# Methods

We tested the ability of ensemble models to improve estimates of population status and trajectory when applied to both a large simulated dataset and a global database of assessed stock status. Here we describe the datasets, individual models of population status, and ensemble models to combine those estimates. We then describe how we evaluated the ability of the various models to estimate population status.

## Datasets

We first developed and tested ensemble models with a fully factorial simulated dataset [@rosenberg2014]. In summary, the simulation models included three life histories: small pelagic, demersal, large pelagic; three levels of how depleted biomass was at the start of the dataset compared to carrying capacity: 100%, 70%, and 40% of carrying capacity; and four exploitation dynamics: a constant exploitation rate, a exploitation rate that is coupled with biomass to mimic an open-access single-species fishery, a scenario where exploitation rate increases to a fixed level, and a 'roller-coaster' scenario where the exploitation rate increases and then decreases. Process noise was introduced to the models through two levels of multiplicative recruitment variability: $N(0, 0.2)$ and $N(0, 0.6)$, and that process noise was either uncorrelated or had a first-order autoregressive correlation of 0.6. The simulation included a scenario without observation error and with multiplicative observation error around catch at $N(0, 0.2)$. 

@rosenberg2014 ran ten iterations for each combination of factors adding stochastic draws of recruitment and catch-recording variability each time to generate a total of 5760 stocks. The simulation models were built in the package FLR [@kell2007] for the statistical software \textsf{R} [@r2015]. @rosenberg2014 provide a full description of the simulation model and the code to generate the simulations is available at <https://github.com/flr/StockSims> (TODO: not there). 

We also tested our ensemble models on the RAM Legacy Stock Assessment Database [@ricard2012]. Our analysis of the stock-assessment database was based on version XX downloaded on XX. After removing stocks for which the models described below did not converge, this database included `r ram_stocks_n` from XX countries.

## Individual models of population status

We fit four individual data-limited models to estimate \bbmsy\\. Three of the models are mechanistic and based generally on @schaefer1954 biomass dynamics of the form

$$\hat{B}_{t+1} = B_t + r B_t \left(1 - B_t / B_0 \right) - C_t,$$

\noindent
where $\hat{B}_{t+1}$ represents predicted biomass at time $t$ plus one year, $B_t$ represents biomass at time $t$, $r$ represents the intrinsic population growth rate, $B_0$ represents unfished biomass or carrying capacity, and $C$ represents catch. @rosenberg2014 provide a full summary of these four methods and code to fit all the models is available in an accompanying package **datalimited** for the statistical software \textsf{R}. In summary:

* *COM-SIR* (catch-only-model with sample-importance-resampling) is a coupled
  harvest-dynamics model developed by @vasconcellos2005. Biomass is assumed to
  follow a Schaefer model and harvest dynamics are assumed to follow a logistic
  model. The model is fit with a sample-importance-sampling algorithm developed
  in @rosenberg2014.

* *CMSY* (catch-MSY) implements a stock-reduction analysis with Schaefer
  biomass dynamics [@martell2013]. It requires a prior distribution for $r$ and
  assigns a prior to the relative proportion of biomass at the end compared to
  unfished biomass (depletion) based on the percentage of maximum catch at the
  end of the time series.

* *SSCOM* (state-space catch-only-model) TODO: Jim -- can you give a ~2 sentence summary?

* *mPRM* (modified panel regression model) is a modified version of the
  panel-regression model used in @costello2012. Unlike the other models
  considered, mPRM is empirical and not mechanistic --- it uses the RAM Legacy
  Stock Assessment database to fit a linear model to a series of
  characteristics of the catch time series and stock with stock-assessed
  \bbmsy\\ as the response. The model used in this paper and @rosenberg2014 is
  modified from the original in that it condenses the life-history categories
  into three categories to match the simulated dataset.
  <!--TODO: what are we doing with the RAM stocks again?-->
  <!--TODO: bias adjustment?-->
  <!--TODO: build in fig:didactic -->

## Additional covariates

Ensemble methods allow us to incorporate additional covariates into our models and potentially leverage interactions between these covariates and individual model predictions. Additional covariates could be, for example, life-history characteristics, information on exploitation patterns, or statistical properties of the data. For simplicity, and to allow us to apply models developed with the simulated dataset to the real-world dataset, we added only one set of additional covariates: spectral properties of the catch time series. Spectral analysis decomposes a time series into the frequency domain and provides a means of describing the statistical properties of the catch series that is independent of time series length and magnitude of catch. We fit spectral models to the scaled catch time series (catch divided by maximum catch) with the `spec.ar` function in \textsf{R} and recorded representative short- and long-term spectral densities of 0.05 and 0.2, which correspond to 20- and 5-year cycles.

## Ensemble models

The individual models we seek to combine with ensemble models provide time series of \bbmsy\\. Therefore, we can use ensemble models to estimate multiple properties of these status time series. Here, we focus on two values: the mean and slope of \bbmsy\\ in the last 5 years. Together, these quantities address the recent state and trajectory of status, both of which may be of management and conservation interest. To avoid undue influence of the end points of the time series on the calculated slope, we measured the slope as the Theil-Sen estimator of median slope [@theil1950].

An ensemble model can combine models through anything from a simple average to a complex non-linear model. Here we chose four ensemble models to compare: an average, a linear model with two-way interactions, a random forest, and boosted regression tree. The general form of our models was

$$
\hat{\theta} = 
\argmin_\theta \left( L(b \, | \, \hat{b}_{\mathrm{ensemble}}) \right)
$$
$$
\hat{b}_\mathrm{ensemble} = 
f \left( \hat{b}_\mathrm{CMSY}, \, \hat{b}_\mathrm{COM-SIR}, \,
\hat{b}_\mathrm{SSCOM}, \, \hat{b}_\mathrm{mPRM}, \,
S(0.2), \, S(0.5), \, | \, \theta \right)
$$

\noindent 
where $L$ is a generalized loss function, $f$ is some generalized regression function, $\hat{b}_\mathrm{CMSY}$ is the mean or slope prediction from the CMSY model etc. (Table \ref{tab:predictors}), $S(0.2)$ and $S(0.05)$ represent the spectral density of the scaled catch series at frequencies of 0.2 and 0.05, $b$ is the mean or slope value the model is fitted to, and $\theta$ is the set of parameters for the regression function. For all ensemble models of mean \bbmsy\\, which is bounded at zero, we fit the models in log space and exponentiated the predictions to derive a median estimate of \bbmsy\\. For the estimates of \bbmsy\\ slope, we fit ensemble models on the natural untransformed scale.

We calculated the average ensemble models as the geometric mean of the individual model \bbmsy\\ estimates and the mean of the individual model \bbmsy\\ slopes. We fit the linear model ensemble with second-order interactions. We fit two machine learning ensemble models based on regression trees. Regression trees sequentially determine what value of a predictor best splits the response data into two 'branches' based on a specified loss function. In random forests, a series of regression trees are built on a random subset of the data and random subset of the covariates of the model (REF). In generalized boosted models (GBMs), each subsequent model is fit to the residuals from the previous model; data points that are fit poorly in a given model are given more weight in the next model [@elith2008]. Random forests and GBMs can provide strong predictive performance and fit highly non-linear relationships [e.g. @elith2008; @hastie2009]. We fit random forest models with the **randomForest** package [@liaw2002] for \textsf{R} with the default argument values. We fit boosted regression tree models with the **GBM** package [@ridgeway2015] for \textsf{R}. Based on cross-validation with the **caret** \textsf{R} package, we fit GBMs with 2000 trees, an interaction depth of $6$, a learning rate (shrinkage parameter) of $0.01$, and all other arguments at their default values.                                                

## Testing model performance

A critical component to any predictive modelling exercise is to evaluate the performance of a model on new data [@hastie2009]. We used repeated three-fold cross validation to test predictive performance: we randomly divided the dataset into three sets, built ensemble models on two-thirds of the data, and evaluated predictive performance on the remaining third. We repeated this across each of the three splits and then repeated the whole procedure 50 times to account for bias that may result from any one set of validation splits. Since the dynamics of simulated populations differing only in the stochastic draw of random values are likely similar, we grouped these stocks (10 per factorial cell) in the cross-validation process into either the training or testing split. Since mPRM is built on the RAM Legacy Stock Assessment database, when testing model performance on the stock assessment database, we refit mPRM on each training split.

Predictive performance can be evaluated with metrics that represent a variety of modelling goals. For continuous response variables such as the mean and slope of population status, performance metrics often measure some form of bias, precision, accuracy (a combination of bias and precision), or the ability to correctly rank or correlate across populations (REFs). Here, we measure proportional error (PE; also called 'relative error') defined as $(\hat{b} - b)/b$. We summarize median proportional error to measure bias, median absolute proportional error to measure accuracy, and Spearman's rank-order correlation to measure the ability to correctly rank populations.

# Results

Applied to the simulated dataset of known status, the individual models had variable success at recovering the mean (status) and slope (trajectory) of \bbmsy\\ in the last five years. All models exhibited a high degree of scatter around the 1:1 line of perfect status prediction (Fig. \ref{hexagon}). CMSY exhibited bimodal predictions of status but had the best rank-order correlation and accuracy scores (Fig. \ref{performance}a). COM-SIR and SSCOM both correctly identified a number of low status stocks, but frequently predicted a high status when status was in fact low (Fig. \ref{hexagon}b, c). mPRM had relatively poor ability to predict status for the simulated dataset (Fig. \ref{hexagon}d). There was generally little correlation between true and predicted trajectory for any of the individual models besides SSCOM (Figs \ref{scatter-sim-slope}a--d, \ref{performance-sim-slope}).

Ensemble models, and in particular the machine learning ensemble models (random forest and GBM), generally improved estimates of status and trajectory (Fig. \ref{hexagon}e--h, Fig. \ref{scatter-sim-slope}e--h). Compared to the individual models, machine learning ensembles improved accuracy (median absolute proportional error) by `r mean_sim$mach_vs_ind_mare_fold`%, increased rank-order correlation from `r mean_sim$ind_corr_range` to `r mean_sim$mach_corr_range`, and reduced bias (median proportional error) from `r mean_sim$ind_mre_range` to `r mean_sim$mach_mre_range` (Figs \ref{performance}a). These ensembles also had better ability to distinguish if simulated stocks were above or below $B/B_\mathrm{MSY} = 1$ (Fig. \ref{roc-sim}). Results were similar when predicting trajectory: compared to individual models, machine learning ensembles improved accuracy by `r slope_sim$mach_vs_ind_mare_fold`%, increased rank-order correlation from `r slope_sim$ind_corr_range` to `r slope_sim$mach_corr_range`, and reduced bias from `r slope_sim$ind_mre_range` to `r slope_sim$mach_mre_range` (Fig. \ref{performance-sim-slope}). The ensemble models that simply took a mean of the individual models ranked slightly behind the best individual model for estimating status (CMSY; Fig. \ref{performance}a) and had slightly lower correlation but higher accuracy than the best individual model at predicting slope (SSCOM; Fig. \ref{performance-sim-slope}).

The ensemble models were able to improve the predictions of status by exploiting the best properties of individual models, the covariance between individual models, and interactions with other covariates. For example, SSCOM had strong predictive ability when it predicted low \bbmsy\\ (Fig. \ref{hexagon}c, Fig. \ref{partial-sim}c) and CMSY predictions were approximately linearly related to \bbmsy\\ within the low and high prediction clusters (Fig. \ref{partial-sim}). Ensembles also exploited the covariance between individual model predictions. For instance, both the linear model and GBM ensemble suggest that if mPRM and SSCOM predict high status, the true status also tends to be high (Figs \ref{lm-coefs}, \ref{partial-2d-sim}l). The addition of spectral density covariates helped the ensemble models correctly predict higher status values (Fig. \ref{hexagon}g, h) by exploiting interactions between the spectral density predictors and the individual model predictions (Fig. \ref{partial-2d-sim}). Although the additional covariates improved ensemble fit, performance of the ensembles was only marginally degraded by removing these covariates (Fig. \ref{hexagon} vs. Fig. \ref{hexagon-sim-basic}) TODO NEW FIG THAT QUANTIFIES THIS.

When applied to the stock assessment database, the ensemble models---trained exclusively on the simulated dataset---generally performed as well or better than the best individual models. The mean, random forest, and GBM ensembles even outperformed the mPRM method which is trained on the RAM Legacy Stock Assessment database itself (Fig. \ref{performance}b, Fig. \ref{hexagon-ram}). Compared to the individual models, the machine learning ensembles increased accuracy by `r mean_ram$mach_vs_ind_mare_fold`%, improved correlation from `r mean_ram$ind_corr_range` to `r mean_ram$mach_corr_range`, and reduced bias from `r mean_ram$ind_mre_range` to `r mean_ram$mach_mre_range`.                    

<!--
Some individual models performed nearly as well as the ensembles in one or two performance dimensions: e.g. SSCOM does nearly as well as the ensembles at predicting the slope of \bbmsy\\ (Fig. \ref{performance-sim-slope}). But no individual method is consistently nearly as good as the ensembles. SSCOM does more poorly than the ensembles and other individual models in terms of accuracy, rank-order correlation, and bias at estimating the mean \bbmsy\\ for both the simulated and stock-assessment datasets (Fig. \ref{performance}).
-->

# Discussion

Ensemble models provide a useful approach to situations where conservation and management decisions must be made on the basis of multiple, potentially contrasting estimates of status---a situation that is common to many ecological settings beyond fisheries. Compared to individual models of fish population status, ensemble models were consistently the best or among the best across three performance dimensions, two response variables, two datasets, and multiple ensemble model types. Our results suggest choosing an ensemble model that allows for non-parametric relationships, such as machine learning methods---these methods provide added insight into individual model behaviour and generally performed the best---but even a simple average of predictions across multiple models may be useful.

Certain conditions will make some ensemble models more effective than others. First, ensemble models will be most effective when they are comprised of diverse individual models that make substantially different assumptions about underlying dynamics, or make different but plausible assumptions about certain parameter values [e.g. @ali1996; @tebaldi2007]. Such models would be expected to perform well across different conditions and an ensemble model can exploit the best predictive performance of each model. Second, ensemble models will be most effective when they are not overfit to training data. The probability of overfitting ensemble models generally increases with the number of individual models [e.g. @caruana2004]. Cross-validation testing, information theoretic methods (e.g. AIC), and methods that are robust to over-fitting such as random forests (REF), may help avoid overfitting ensemble models. We note that our simplest ensemble model, a mean of the individual model predictions, performed approximately as well as the complex machine learning models when we applied our ensembles to an entirely new dataset (FIG). Third, ensemble models will be most effective when they are trained on data that are representative of the dataset of interest. Cross-validation within a training dataset will provide a optimistically biased impression of predictive performance if the training dataset fundamentally differs from the dataset of interest.

Ensemble models share similarities with coefficient averaging (Burnham and Anderson REF) but differ in many important way. At a high level, ensemble models and coefficient averaging share the same idea that no one model is correct and multiple models can contribute useful information for inference. A fundamental difference between coefficient averaging and ensemble models is that coefficient averaging focuses on a averaging *coefficients* whereas ensembles instead average *predictions*. By focussing on predictions, ensembles are a more general purpose tool: they don't require information theoretics, they can combine completely different types of models (e.g. combining parametric and non-parametric models or frequentist and Bayesian predictions), and they can combine predictions in complex non-linear ways that also depend on additional covariates.

A strength of ensemble models is that they can be tailored or specific response variables. For example, we built separate models of mean \bbmsy\\ and the slope of \bbmsy\\. The same set of model weights or non-linear relationships need not hold across different response variables. Ensemble models can optimally combine the individual models depending on the response. Ensemble models could be combined to estimate any number of response variables of interest. For example, they could be combined to estimate a binary response such as whether status is above or below some threshold.

* Coilin: How ensembles can help understand mechanistic underpinnings of individual 
  models
    * discover surprises: counterintuitive relationships and interactions
    * can dig into surprising conditions and learn about why certain models fail; 
      potentially learn how to improve models
    * can learn under what conditions certain models perform well
    * pick out an example

* Jim: Other possible applications of ensemble models outside fisheries status and
  trajectory
    * other taxa examples
    * other kinds of response variables
	
<!--
* Other possible topics:
    * how much better are the ensembles really? translate the results into
      some concrete examples in an absolute not relative sense
    * other types of ensembles: assigning weights, incorporating uncertainty
      around estimates, GAMs, ...
    * when wouldn't you want to use ensembles? perhaps if models represent 
      dichotomous assumptions where either one or the other is right and imply
      different management actions? ...
    * your ideas here...
-->
	
# References

# Acknowledgements

<!--Funding...-->

<!--# Citation notes-->

<!--
- @breiner2015 ensemble models of species distribution models for rare species

- @jones2015 ensemble models of species distribution models - globally for
  marine biodiversity

- [@greene2006] example similar to ours but with climate models (Bayesian
  multilevel ensemble)

- [@kell2007] FLR

- multiple learners book [@alpaydin2010]

- [@caruana2004] nice paper on ensemble models; prob of overfitting increases
  with more models, bagging important

- [@tebaldi2007] key paper: review of 'multi-model ensembles for climate projections'

- famous ensemble is DEMETER: Development of a European Multi-model Ensemble
  System for Seasonal to Interannual Prediction @hagedorn2005 is a good
  reference

- Examples of where ensemble models are shown to be better than any one:
  @thomson2006 (public health - malaria), @cantelaube2005 (agriculture - crop
  yield) (citations taken from @tebaldi2007)

- simple averages are used very commonly in climate science - e.g. IPCC 2001

- @tebaldi2007: weighting obviously makes sense, but how do we define a
  performance metric that is based on past observations that is relevant to the
  future? 

- @tebaldi2007: model independence important

- for climate, weighted averages perform better than simple averages [@min2006],
  but are those same weights applicable to the future (or in our case other
  fisheries)?

- error cancellation is one but not the only reason for superiority of ensemble
  models [@hagedorn2005]

- ensemble model may be only marginally better than the best single model in
  any given case, but we don't usually know which is the best single model
  [@hagedorn2005]

- ensemble models useful for regional climate models too; as an example,
  @pierce2009 use 42 metrics to characterize model performance in regional
  climate model ensembles; found that ensemble models were superior to any one
  model --- especially when considering multiple metrics

- @knutti2009: key review paper on motivation and challenges of combining
  climate projection models; performance on testing / current data may only
  weakly relate to future / other datasets

- @murphy2004: Nature paper on ensembles of climate model simulations; weighted
  average better performance than unweighted average

- @dietterich2000: highly cited book chapter "Ensemble Methods in Machine
  Learning"; ensembles are often much more accurate than the "individual
  classifiers that make them up"; but components must be diverse and accurate

- @dietterich2000: a main justification for ensembles: they are
  representational --- no one model usually can contain all hypothetical
  functional forms, but many separate models can cover more hypotheses

- strong paper showing that ensemble models are most accurate when the various
  individual models make errors in uncorrelated ways [@ali1996]

- without substantial training-testing data, appropriate model weights can be
  very hard to deduce and can cause more harm than good (compared to equal
  weighting) [@weigel2010] (they give the example of seasonal forecasting where
  a ton of hind cast testing can be done, vs. long-term climate) (asymmetrical
  loss function)

- [@weigel2010] optimal weights are always as good or better than equal
  weights, but if you get the weights wrong, you can be better off just using
  equal weights; but averaging was almost always better than any one model

- @rykiel1996 "Testing ecological models: the meaning of validation" (see for
  performance criteria, theory on model testing and assessment)
-->
