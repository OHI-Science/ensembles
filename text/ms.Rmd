<!-- TODO
- [ ] add discussion of uncertainty (CIs) and ML (and incorporating uncertainty into pred.)
- [x] add back in the roc plot and sentence in the main text
- [x] rerun the main simulation analysis because the equations of change for the slope
- [ ] manually adjusted the labels in figure 4
- [x] update the magic number get inserted to use the equation that only has the basic model
- [ ] check figure order
- [ ] check all remaining in-text todo items
-->

```{r, echo=FALSE}
# Values to use in the paper:
load("values.rda") # generated by `../analysis/9-output-values.R`

# Includes: mean_sim, mean_ram, slope_sim, auc_sim, ram_stocks_n

# mean_sim, mean_ram, and slope_sim have the following list elements:
# > names(mean_sim)
# "mach_vs_ind_mare_fold" "ind_corr_range" "mach_corr_range" "ind_mare_range"
# "mach_mare_range" "ensemble_mre_range" "ind_mre_range"

# auc_sim has the following list elements:
# > names(auc_sim)
# "mach_range" "ind_range"
```

<!--
 The full title of the paper, with two alternatives. Keep titles short and simple.
• The full names of all authors.
• The name(s) and address(es) of the institution(s) at which the work was
carried out. If the present address of any author is different from the above,
give it as a footnote.
• The name, address, telephone, fax and e-mail contacts of the author to whom
all correspondence and proofs should be sent.
• A suggested running title of not more than 40 characters, including spaces.
-->

\begin{Large}
\noindent
Improving estimates of population status and trend with superensemble models
\end{Large}

\noindent
Alternative 1: Combining stock-assessment output with ensemble modelling

\noindent
Alternative 2: Ensembles of data-limited stock assessments improve accuracy
and reduce bias of $B/B_\mathrm{MSY}$ estimates

\bigskip

\noindent
Sean C. Anderson^1\*^,
Andrew B. Cooper^1^,
Olaf P. Jensen^2^,
C\'{o}il\'{i}n Minto^3^,
James T. Thorson^4^,
Jessica C. Walsh^1^,
Jamie Afflerbach^5^,
Mark Dickey-Collas^6,7^,
Kristin M. Kleisner^8^,
Catherine Longo^9^,
Giacomo Chato Osio^10^,
Daniel Ovando^11^,
Iago Mosqueira^10^,
Andrew A. Rosenberg^12^,
Elizabeth R. Selig^13^

\bigskip

\noindent
^1^School of Resource and Environmental Management,
Simon Fraser University, Burnaby, BC, V5A 1S6, Canada

\noindent ^2^Institute of Marine & Coastal Sciences, Rutgers University,
71 Dudley Road, New Brunswick, NJ, 08901-8525, USA

\noindent ^3^Marine and Freshwater Research Centre, Galway-Mayo Institute of
Technology, Galway, 00000, Ireland

\noindent ^4^Fisheries Resource Analysis and Monitoring Division, Northwest
Fisheries Science Center, National Marine Fisheries Service, National
Oceanographic and Atmospheric Administration, 2725 Montlake Boulevard E.,
Seattle, WA, 98112, USA

\noindent ^5^National Center for Ecological Analysis and Synthesis, University
of California Santa Barbara, 735 State Street, Suite 300, Santa Barbara, CA,
93103, USA

\noindent ^6^International Council for the Exploration of the Sea, H.C.
Andersens Boulevard 44-46, DK 1553, Copenhagen, Denmark

\noindent ^7^DTU Aqua National Institute of Aquatic Resources, Technical
University of Denmark (DTU), Jægersborg Alle 1, 2920 Charlottenlund, Denmark

\noindent ^8^Ecosystem Assessment Program, Northeast Fisheries Science Center,
National Marine Fisheries Service, National Oceanographic and Atmospheric
Administration,
166 Water St., Woods Hole, MA, 02543, USA

\noindent ^9^Marine Stewarship Council, Marine House, 1 Snow Hill, London, EC1A
2DH, United Kingdom

\noindent ^10^European Commission, Joint Research Centre, Institute for the
Protection and Security of the Citizen, Maritime Affairs Unit G03, Via E. Fermi
2749, 21027 Ispra (VA), Italy

\noindent ^11^Bren School of Environmental Science and Management, University of
California Santa Barbara, Santa Barbara, CA, 93106-5131, USA

\noindent ^12^Union of Concerned Scientists, Cambridge, MA, USA

\noindent ^13^Conservation International, Arlington, VA, USA

\noindent ^\*^Corresponding author, present address: School of Aquatic and
Fishery Sciences, Box 355020, University of Washington, Seattle, WA  98195, USA
E-mail: sandrsn@uw.edu

\bigskip

\noindent Running title: Superensembles of population status



\clearpage
<!--up to 250 words-->

\bigskip

\noindent \textbf{Abstract:} Fishery managers must often reconcile conflicting
estimates of population status and trend. One solution is to average
predictions using an ensemble method, but this approach ignores covariance
between models and may not optimally reduce bias and improve accuracy.
Superensemble models, commonly used in climate and weather forecasting, may
provide a superior solution. Superensembles use predictions from multiple
models as covariates in an additional model. By training this superensemble
model using known data, we can then use it to predict unknown situations. We
evaluated the potential for ensemble and superensemble models ('ensemble
methods') to improve estimates of population status and trend for fisheries.
We fit four widely applicable data-limited models that estimate stock biomass
relative to the equilibrium biomass at maximum sustainable yield
($B/B_\mathrm{MSY}$). We combined the models' estimates to build four ensemble
methods: an ensemble average and three superensembles (a linear model,
a random forest, and a boosted regression tree). We used the ensemble methods
to predict both (i) fishery status and (ii) recent trends of
$B/B_\mathrm{MSY}$. We trained our models on a simulated dataset of 5760
stocks and tested them with cross-validation and against a global database of
`r ram_stocks_n` real stock assessments. We found that ensemble methods
substantially improved estimates of population status and trend. Random forest
and boosted regression trees performed the best for estimating population
status: accuracy improved
`r mean_sim$mach_vs_ind_mare_fold`%, rank-order
correlation between predicted and true status improved from
`r mean_sim$ind_corr_range` to
`r mean_sim$mach_corr_range` across models, and
bias (median proportional error) declined from
`r mean_sim$ind_mre_range` to
`r mean_sim$mach_mre_range`. We found similar improvements when predicting the
trend of status and when applying the simulation-trained superensembles to
catch data for global fish stocks. Ensemble methods can improve estimates of
status and trends; however, they must be tested, formed from a diverse set of
accurate models, and built on a dataset representative of the populations to
which they are applied. *TODO: Still need to cut ~30 words*

\bigskip

\noindent Keywords: data-limited fisheries, ensemble methods, multi-model
averaging, population dynamics, sustainable resource management

\tableofcontents


# Introduction

Status and trend are two of the most fundamental values to quantify in the
management of ecological populations [e.g. @hutchings2010; @iucn2015]. However,
managers are often faced with reconciling multiple uncertain and potentially
conflicting estimates of status and trend [e.g. @brodziak2010; @branch2011;
@deroba2015]. For example, one model may suggest a population is at risk and
declining in abundance while others may suggest it is not at risk and stable.

One solution is to take the average or weighted average of several model
predictions, i.e. an ensemble. Such ensembles are typically more accurate and
less biased than individual model estimates and can integrate uncertainty in
model structure, initial conditions, and parameter estimation
[@dietterich2000; @araujo2007]. Ensembles are superior to individual models
in at least three ways: (1) statistically by averaging across models and
therefore being less likely to pick the "wrong" model, (2) computationally by
reducing the risk of getting stuck in a local optima, and (3)
representationally by expanding the range of hypotheses explored
[@dietterich2000]. This approach forms the basis of many machine learning
methods [e.g. @dietterich2000], has helped reconcile climate forecasts from
dozens of models [e.g. @murphy2004; @tebaldi2007; @ipcc2013], and even
improved early warning signs of malaria outbreaks [@thomson2006]. In ecology,
ensemble methods are sometimes used to improve species distribution modelling
[e.g. @araujo2007; @breiner2015] and indeed have been used to combine
estimates of population status and trend [e.g. @brodziak2010].

Whereas averages or weighted averages of model estimates may improve
predictions compared to a single model, they may not optimally leverage
available data. The best prediction does not necessarily lie in the middle of
multiple model predictions, some models may perform better than others in
certain conditions, and the covariance between models may contain information
that can improve predictive accuracy. For example, one model might perform
well at estimating high levels of abundance but be biased at low levels of
abundance, while another model might have the opposite properties. An optimal
combination of these models is not simply an average of the two.

We can exploit these characteristics by using the predictions from a group of
models as inputs into a separate statistical model. This technique, sometimes
called superensemble modelling [@krishnamurti1999], is common in climate and
weather forecasting [e.g. @yun2005; @mote2015]. The superensemble is fit to a
training dataset where outcomes are well known and then used to predict on a
dataset of interest. For example, @krishnamurti1999
combined predictions of wind and precipitation in Asian monsoons via a
superensemble regression fit to observed data. Their superensemble was
considerably more accurate than any individual prediction or weighted average
of predictions.

In fisheries science, the commonly used operational models for determining
status and trend of exploited fish populations are stock assessments, i.e.,
population models coupled to an observation model, that incorporate all
appropriate data (e.g. catches, size and age distributions, surveys, and
tagging information) to quantify values such as the biomass of a stock that
can produce maximum sustainable yield ($B_\mathrm{MSY}$) [@hilborn1992].
However, the broad range of data required to conduct these stock assessments
are not available for the majority of fish populations, including those of
conservation concern and of economic interest to fisheries [@fao2014].
Therefore, a number of models have been proposed to assess $B/B_\mathrm{MSY}$
based on the limited data available for the majority of fish stocks: (1) a time
series of the total biomass of catch and (2) a basic understanding of
population productivity [e.g. @vasconcellos2005; @martell2013]. Recently,
@rosenberg2014 investigated the performance of four data-limited models through
a large-scale simulation experiment. Three of these models were based on
Schaefer (logistic) biomass dynamics and one was an empirical model fitted to
more data-rich stock-assessment output. The four models frequently disagreed
about population status (e.g. Fig. \ref{motivate}), no one model had strong
performance across all fish stocks, and some models performed better than
others depending on circumstances.

Here, we estimate population status and trend of exploited fish populations
using ensembles and superensembles (collectively 'ensemble methods') of these
four data-limited models. We apply four ensemble-method approaches of varying
complexity to both simulated and real-world fish stocks and compare their
predictive performance against each other and the individual models.

# Methods

<!--TODO Add in the reference to (Fig. \ref{didactic}) and reorder the figures and add
in the figure with the partial dependence -->

## Individual models of population status

We fit four individual data-limited models that use catch data and basic
life-history parameters to estimate $B/B_\mathrm{MSY}$. We chose these models
because they can be fit to the vast majority of fisheries around the
world, are established in the literature, and have been extensively simulation
tested [@rosenberg2014].

Three of the models are mechanistic and based generally on Schaefer biomass
dynamics [@schaefer1954] of the form

$$\hat{B}_{t+1} = B_t + r B_t \left(1 - B_t / B_0 \right) - C_t,$$

\noindent where $\hat{B}_{t+1}$ represents predicted biomass at time $t$ plus
one year, $B_t$ represents biomass at time $t$, $r$ represents intrinsic
population growth rate, $B_0$ represents unfished biomass or carrying capacity
$K$, and $C$ represents catch. The fourth model is an empirically derived model
based on the RAM Legacy Stock Assessment Database [@ricard2012]. @rosenberg2014
provide a full background on these four methods and code to fit all the models
is available in an accompanying package `datalimited` for the statistical
software R [@r2015] (<https://github.com/datalimited/datalimited>) (*Private while in
review*). In summary:

* *CMSY* (catch-MSY) implements a stock-reduction analysis with Schaefer
  biomass dynamics [@martell2013]. It requires a prior distribution for $r$
  and a prior on the relative proportion of biomass at the end of the time
  series compared to unfished biomass (depletion), which is derived from the
  percentage of maximum catch at the end of the time series. The version of
  the model used in @rosenberg2014 was modified from @martell2013 to generate
  biomass trends from all viable $r$-$K$ pairs and produce an estimate of
  $B/B_\mathrm{MSY}$ from the median trend.

* *COM-SIR* (catch-only-model with sampling-importance-resampling) is
  a coupled harvest-dynamics model [@vasconcellos2005]. Biomass is assumed to
  follow a Schaefer model and harvest dynamics are assumed to follow
  a logistic model. The model is fit with a sampling-importance-resampling
  algorithm [@rosenberg2014].

* *SSCOM* (state-space catch-only model) is a hierarchical model that, similar
  to COM-SIR, is based on a coupled harvest-dynamics model [@thorson2013].
  SSCOM estimates unobserved dynamics in both fishing effort and the fished
  population based on a catch time series and priors on $r$, the maximum rate
  of increase of fishing effort, and the magnitude of various forms of
  stochasticity. The model is fit in a Bayesian state-space framework to
  integrate across three forms of stochasticity: variation in effort,
  population dynamics, and fishing efficiency [@thorson2013].

* *mPRM* (modified panel regression model) is a modified version of the
  panel-regression model from @costello2012. Unlike the other models, mPRM is
  empirical and not mechanistic --- it uses the RAM Legacy Stock Assessment
  database to fit a regression model to a series of characteristics of the
  catch time series and stock with stock-assessed $B/B_\mathrm{MSY}$ as the
  response. The model used in this paper is modified from the original --- it
  condenses the life-history categories into three categories to match the
  simulated dataset, removes the maximum catch predictor since the absolute
  catch in the simulated dataset is arbitrary, and does not implement the bias
  correction needed in @costello2012 since we do not derive estimates of
  median status across multiple stocks.

## Simulated dataset to build the superensemble

We first developed and tested ensemble methods on a fully factorial simulated
dataset of fisheries [@rosenberg2014] with known status. These simulated data
included the following scenarios: three fish life histories: small pelagic,
demersal, and large pelagic; three levels of initial biomass depletion
compared to carrying capacity: biomass at 100%, 70%, and 40% of carrying
capacity; and four exploitation settings: (1) a constant exploitation rate,
(2) an exploitation rate coupled with biomass to mimic an open-access
single-species fishery, (3) a scenario where exploitation rate increased
continuously, and (4) a "roller-coaster" scenario where the exploitation
rate increased and then decreased. Process noise (recruitment variability;
i.e. unexplained variability in population dynamics) was introduced to the
models at two magnitudes in log space, $N(0, 0.2^2)$ and $N(0, 0.6^2)$,
and was either uncorrelated through time or had first-order autoregressive
correlation of $0.6$. The simulation also included a scenario with $N(0,
0.2^2)$ measurement error around log(catch) and one scenario without
measurement error. @rosenberg2014 ran ten iterations for each combination
of factors adding stochastic draws of recruitment and catch-recording
variability each time to generate a total of 5760 stocks. The simulation
models were built in the `FLR` packages [@kell2007,
<http://flr-project.org>] for the statistical software R. Code to
generate the simulations is available at
<https://github.com/iagomosqueira/stockStatusFAO>.

## Building the superensemble models

The individual models we seek to combine with superensembles provide time
series of stock status ($B/B_\mathrm{MSY}$). Therefore, we can use
superensembles to estimate any property of these time series. Here, we focus
on two properties: the mean and slope of $B/B_\mathrm{MSY}$ in the last five
years. Together, these quantities address the recent state and trend of stock
status, which are both of management and conservation interest [e.g. @hutchings2010; @iucn2015]. To avoid undue influence of the time series end points
on the calculated slope, we measured the slope as the Theil-Sen estimator of median slope [@theil1950].

We used the mean or slope of $B/B_\mathrm{MSY}$ as the response variable and
the predictions from the individual models as predictors in our superensemble
models. When modelling mean $B/B_\mathrm{MSY}$ --- a ratio bounded at zero ---
we fit the superensemble models in log space and exponentiated the
predictions. For the estimates of $B/B_\mathrm{MSY}$ slope, which are not
bounded at zero, we fit superensemble models on the natural untransformed
scale.

TODO: standardize notation of the following with figure 2

We compared an ensemble average and three superensembles of varying
complexity: a linear model with two-way interactions, a random forest, and a
boosted regression tree. We describe these models as estimating
$\hat{\theta}$, which represents either the ensemble estimated $\log
B/B_\mathrm{MSY}$ or slope of $B/B_\mathrm{MSY}$. The individual model
estimates of $\log B/B_\mathrm{MSY}$ or slope of $B/B_\mathrm{MSY}$ are
represented as $\hat{b}$ for models $i$ $1$ through $4$ (CMSY, SSCOM, COM-SIR, mPRM).
The ensemble average for each fishery $i$ was calculated as:

\begin{equation}
\hat{\theta_i} = \left( \hat{b}_{i,1} + \hat{b}_{i,2} + \hat{b}_{i,3} + \hat{b}_{i,4} \right) / 4, \quad
\mathrm{for}\ i  =  1,...,n.
\end{equation}

\noindent We fit the linear model superensemble with all second-order
interactions:

\begin{equation}
\hat{\theta_i} = \beta_0 + \beta_1 \hat{b}_{i,1} + ... +
\beta_{1,2} \hat{b}_{i,1} \hat{b}_{i,2} + ... +
\epsilon_i,
\quad
\epsilon \sim \mathrm{Normal}(0, \sigma^2),
\quad \mathrm{for}\ i  =  1,...,n.
\end{equation}

Our two machine learning superensemble models, a random forest and a
generalized boosted model (GBM), were based on regression trees. Regression
trees sequentially determine what value of a predictor best splits the
response data into two branches based on a loss function [@breiman1984]. In
random forests, a series of regression trees are built on a random subset of
the data and random subset of the covariates of the model [@breiman2001]. In
GBMs, each subsequent model is fit to the residuals from the previous model;
data points that are fit poorly in a given model are given more weight in the
next model [@elith2008]. Random forests and GBMs can provide strong predictive
performance and fit highly non-linear relationships [@elith2008; @hastie2009].
We fit random forest models with the `randomForest` package [@liaw2002] for R
with the default argument values. We fit boosted regression tree models with
the `gbm` package [@ridgeway2015] for R. We fit GBMs with 2000 trees, an
interaction depth of $6$, a learning rate (shrinkage parameter) of $0.01$, and
all other arguments at their default values.

## Additional covariates

Superensemble models allow us to incorporate additional covariates and
potentially leverage interactions between these covariates and individual model
predictions. Additional covariates could be, for example, life-history
characteristics, information on exploitation patterns, or statistical
properties of the data. We tested the performance benefits of including one set
of additional covariates: spectral properties of the catch time series.
Spectral analysis decomposes a time series into the frequency domain and
provides a means of describing the cyclical shape of the catch series that is
independent of time series length (except in affecting precision) and
independent of absolute magnitude of catch. We fit spectral models to the
scaled catch time series (catch divided by maximum catch) with the `spec.ar`
function in R and recorded representative short- and long-term spectral
densities at frequencies of 0.20 and 0.05, which correspond to 5- and 20-year
cycles. We include the results of adding these additional covariates in the
supplementary materials.

TODO add short description of spectral densities

## Applying the superensemble models and testing performance

Once the superensemble models are built, we can apply them to new stocks that
we'd like to know the status of. To do this we applied the individual models
to our stocks of interest and then applied these data to our already built
superensemble models. In this paper we applied the superensemble models to
subsets of the simulated data as a cross-validation test and to the RAM Legacy
Stock Assessment database to test on real stocks.

We used repeated three-fold cross validation to test predictive performance: we
randomly divided the dataset into three sets, built superensemble models on
two-thirds of the data, and evaluated predictive performance on the remaining
third. We repeated this across each of the three splits and then repeated the
whole procedure 50 times to account for bias that may result from any one set
of validation splits. In the simulated dataset, there were 10 replicates of
each unique combination of simulation parameters that differed only in
stochastic variability. Since the dynamics of these populations were often
similar, we grouped these stocks in the cross-validation process into either
the training or testing split.

We also tested our ensemble methods on the RAM Legacy Stock Assessment Database
--- a compilation of stock-assessment output from hundreds of exploited marine
populations around the world. Our analysis of the stock-assessment database was
based on version 2.5. After removing stocks for which at least one of the
individual models described below did not converge (121), this database
included `r ram_stocks_n` stocks. We removed these stocks for all methods ---
both for the individual and superensemble models. An alternative would be to
fit superensemble models to subsets of the individual models that did converge,
but for simplicity we will only use superensemble models fitted to all four
individual models.

In the case of the RAM Legacy Stock Assessment Database, we used superensembles
trained on the entire simulation dataset. However, since mPRM is built on the
same stock-assessment database, we applied three-fold cross-validation to the
data underlying the mPRM model so that the dataset with which mPRM was trained
(for the individual model and superensemble) was separate from the dataset with
which it was tested.

Predictive performance can be evaluated with metrics that represent a variety
of modelling goals. For continuous response variables such as the mean and
slope of population status, performance metrics often measure some form of
bias, precision, accuracy (a combination of bias and precision), or the ability
to correctly rank or correlate across populations [e.g. @walther2005]. Here, we
measure proportional error, defined as $(\hat{\theta} - \theta)/\abs{\theta}$,
where $\hat{\theta}$ and $\theta$ represent estimated and "true" (or
stock-assessed) mean or slope of $B/B_\mathrm{MSY}$. We calculated median
proportional error to measure bias, median absolute proportional error to
measure accuracy, and Spearman's rank-order correlation between predicted and
"true" values to measure the ability to correctly rank populations. When
testing with the RAM Legacy Stock Assessment database, we treated the
estimates from these data-rich stock assessments as known without error. Thus,
any error in the stock-assessment estimates of the mean or slope of
$B/B_\mathrm{MSY}$ also contributes to our estimates of prediction error for
each of the four data-limited models and the ensembles. Code to reproduce our
analysis is available at <https://github.com/datalimited/ensembles> (*Private while in
review*).

# Results

Applied to the simulated dataset of known stock status, the individual models
had variable success at recovering the mean (status) and slope (trend) of
$B/B_\mathrm{MSY}$ in the last five years. All models exhibited a high degree of
scatter around the one-to-one line of perfect status prediction (Fig.
\ref{hexagon}). In contrast to the known unimodal distribution of status, CMSY
exhibited bimodal predictions (Fig. \ref{hexagon}a), but had the best rank-order
correlation and accuracy scores (Fig. \ref{performance}a). COM-SIR and SSCOM
both correctly identified a number of stocks with low status, but frequently
predicted a high status when status was in fact low (Fig. \ref{hexagon}b, c).
mPRM had relatively poor ability to predict status for the simulated dataset
(Fig. \ref{hexagon}d). There was generally little correlation between true and
predicted recent trend in status for any of the individual models (rank-order
correlation = `r range_slope_corr_non_sscom`) with the exception of SSCOM
(correlation = `r slope_corr_sscom`; Figs \ref{scatter-sim-slope}a--d).

Ensemble methods, and in particular the machine learning superensemble models
(random forest and GBM), generally improved estimates of status and trend over
any individual model (Fig. \ref{hexagon}e--h, Fig. \ref{scatter-sim-slope}e--h).
Compared to the individual models, machine learning superensembles improved
accuracy (median absolute proportional error) by
`r mean_sim_basic$mach_vs_ind_mare_fold`%, increased rank-order correlation from
`r mean_sim_basic$ind_corr_range` to `r mean_sim_basic$mach_corr_range`, and reduced bias
(median proportional error) from `r mean_sim_basic$ind_mre_range` to
`r mean_sim_basic$mach_mre_range` (Fig. \ref{performance}a).
These superensembles also generally had better ability to distinguish if
simulated stocks were above or below $B/B_\mathrm{MSY} = 0.5$ (Fig. \ref{roc-sim}).
Results were similar when predicting trend: compared to individual models,
machine learning superensembles improved accuracy by
`r slope_sim$mach_vs_ind_mare_fold`%, increased rank-order correlation from
`r slope_sim$ind_corr_range` to `r slope_sim$mach_corr_range`, and reduced
bias from `r slope_sim$ind_mre_range` to `r slope_sim$mach_mre_range` (Fig.
\ref{performance-sim-slope}).
The ensemble models that simply took a mean of the individual models ranked
slightly behind the best individual model for estimating fish stock status
(CMSY; Fig. \ref{performance}a) and had slightly lower correlation but higher
accuracy than the best individual model at predicting the trends of status
(SSCOM; Fig. \ref{performance-sim-slope}).

The superensemble models were able to improve the predictive performance by
harnessing the best properties of individual models, the covariance between
individual models, and interactions with other covariates. For example, SSCOM
had strong predictive ability when it predicted low $B/B_\mathrm{MSY}$ (Fig.
\ref{hexagon}c, Fig. \ref{partial-sim}c) and CMSY predictions were
approximately linearly related to $B/B_\mathrm{MSY}$ within the low and high
clusters of predictions (Fig. \ref{partial-sim}). SSCOM contributed most strongly on its own to determining trend (Fig. \ref{partial-sim-slope}). Superensembles also exploited
the covariance between individual model predictions. For instance, both the
linear model and GBM ensemble suggest that if mPRM and SSCOM predict high
status, the true status also tends to be high (Figs \ref{lm-coefs},
\ref{partial-2d-sim}l). The addition of spectral density covariates helped the
superensemble models correctly predict higher status values (Fig.
\ref{hexagon-sim-covariates}g, h). The performance of the ensembles was only
marginally improved by including these covariates (Fig.
\ref{performance-with-covariates} vs. Fig. \ref{performance}).

When applied to the stock-assessment database, the superensemble models ---
trained exclusively on the simulated dataset --- generally performed as well or
better than the best individual models. The mean, random forest, and GBM
ensembles outperformed the mPRM method which is trained directly on the RAM
Legacy Stock Assessment database itself (Fig. \ref{performance}b, Fig.
\ref{hexagon-ram}). Compared to the individual models, the machine learning
superensembles increased accuracy by
`r mean_ram$mach_vs_ind_mare_fold`%, improved correlation from
`r mean_ram$ind_corr_range` to `r mean_ram$mach_corr_range`, and reduced bias
from `r mean_ram$ind_mre_range` to `r mean_ram$mach_mre_range`.

# Discussion

<!-- Dan: Lastly, it's worth noting that this approach, as laid out here is for
helping improve potentially conflicting estimates of the same metric, B/Bmsy. In
many cases though a fishery is going to be basing its decisions on streams of
multiple indicators, e.g trends in SPR and B/Bmsy. While the ensemble methods
here can help refine estimates of individual metrics, it's still worth looking
at what alternative data streams are telling you if possible. E.g. if a purely
catch based B/Bmsy estimate is high and has a positive slope, but mean lengths
and SPR are decreasing, managers will still need to think about how to deal with
these conflicting signals. So, this ensemble method, as I interpret it, is
helpful for resolving conflicts within metric, but doesn't necesarily resolve
conflicts across metrics (if I'm understanding these methods right) -->

Ensemble methods provide a useful approach to situations where environmental
resource management decisions must be made on the basis of multiple,
potentially contrasting estimates of status. Compared to individual models of
fish population status, ensemble methods were consistently the best or among
the best across three performance dimensions (accuracy, bias, and rank-order
correlation), two response variables (status and trend), two datasets
(simulated and global fisheries), and multiple ensemble methods (from a simple
average to machine learning superensembles). Our results suggest choosing a
superensemble model that allows for non-linear relationships, such as
machine learning methods. These methods provide added insight into individual
model behavior and generally performed the best; however, even a simple average
of predictions across multiple models may provide a more useful metric than a single model in many cases.

Certain conditions will make some ensemble models more effective than others.
First, ensembles will be most effective when they are comprised of diverse
individual models that choose different structural model forms, explore
contrasting but plausible ranges of parameter values, and make uncorrelated
errors [@ali1996; @dietterich2000; @tebaldi2007]. We would expect such models
to perform well in different conditions and an ensemble model can
exploit the best predictive performance of each. Second,
ensemble models will be most effective when they are not overfit to the
training dataset. Cross-validation testing [@caruana2004; @hastie2009] and
methods that are robust to overfitting such as random forests [@breiman2001],
may help avoid overfitting ensemble models. We note that our simplest ensemble
model, an average of individual model predictions, performed approximately as
well as complex machine learning models when we trained our superensembles on
the simulation dataset and tested them on a separate "real" dataset (i.e. the
RAM Legacy Stock Assessment database, Fig. \ref{performance}b). Third, ensemble
models will be most effective when they are trained on data that are
representative of the dataset of interest [@knutti2009; @weigel2010].
Cross-validation within a training dataset will provide an optimistically
biased impression of predictive performance if the training dataset
fundamentally differs from the dataset of interest [@hastie2009].

Multi-model inference in the form of coefficient averaging weighted by
information theoretics such as the Akaike Information Criterion (AIC) is
a common analytical approach in fisheries and ecology [e.g. @burnham2002; @johnson2004;
@grueber2011]. The ensemble methods described in this paper share similarities
with coefficient averaging but differ in other important ways. Ensemble methods
and coefficient averaging share the long-held notion that multiple working
hypotheses can contribute useful information for inference [@chamberlin1890].
A fundamental difference is that coefficient averaging focuses on averaging
*coefficients* whereas ensembles instead average *predictions*. Thus, ensembles
provide a general purpose tool: they do not require information theoretics
and they can combine different types of models (e.g. parametric and
non-parametric models or frequentist and Bayesian predictions). Furthermore,
superensembles extend these benefits by allowing model predictions to be
combined via non-linear functions that are tuned to known data.

A strength of superensembles is that they can be tailored to predict specific
response variables. For example, we built separate superensemble models of mean
$B/B_\mathrm{MSY}$ and the slope of $B/B_\mathrm{MSY}$. The same set of model
weights or non-linear relationships need not hold across different response
variables. For instance, SSCOM contributed little to the GBM superensemble
estimate of status at higher levels of predicted $B/B_\mathrm{MSY}$ (Fig.
\ref{partial-sim}), but contributed strongly to estimates of trend (Fig.
\ref{partial-sim-slope}). Formally, fitting superensemble models to specific
quantities of interest provides an additional calibration step [@rykiel1996].
Individual models are typically calibrated (i.e. parameters are estimated) based
on a single response variable; however, this model might then provide a biased
or inaccurate estimate of other quantities of interest. Ensemble methods provide
a final calibration step to a quantity of interest. This ensemble calibration
could further include a loss function tailored to the goals of the model, say
placing greater weight on accuracy at lower rather than higher status levels.
Conversely, because superensembles are tailored to a specific response and loss
function, superensembles force a modeller to choose an operational purpose for
their model up front [*sensu* @dickey-collas2014].

As @box1987 noted, all models are wrong, but some may still be useful. The
ensemble methods we investigated attempt to piece together the useful parts of
candidate models to build a model with improved performance. Instead of viewing
the superensemble as a black box, we think considerable mechanistic
understanding can be gained by studying its structure. For example, when SSCOM
estimates low status this is likely the case, conversely when COMSIR
estimates low status, the true status is more likely to be high (Fig.
\ref{partial-sim}). These models have two main differences: (1) the form of
effort dynamics and (2) the allowance for both measurement and process error in
SSCOM, whereas the implemented COMSIR admits measurement error only. Were the
methods to differ only in effort dynamics, the results point towards a more
suitable representation of effort dynamics at low biomasses in SSCOM. We think
that such investigation of the structure of a superensemble may lead to
improvement in the mechanisms assumed in the individual models.

Combining predictions from multiple models via superensemble methods is
broadly useful in other subfields of fisheries science and ecology in general.
Predictions about extinction risk are widely used at the national (e.g. the US Endangered
Species Act and the Canadian Species at Risk Act) and international [e.g. the
IUCN Red List, @iucn2015] levels. These risk assessments generally involve
fitting regression models to outcomes for individual species along with
predictors of extinction risk [e.g. @anderson2011a; @pinsky2011], or fitting population-dynamic models to data for individual species [e.g. @dfo2010]. Both types of
models are prone to error caused by model-misspecification and therefore results
are sensitive to decisions about model structure [@brooks2015]. Although there
are options to account for potential model-misspecification in determination of
species risk [e.g. coefficient averaging, @burnham2002; generalized modeling,
@yeakel2011; or semi-parametric methods, @thorson2014], ensemble methods are
a relatively simple way to combine predictions in a transparent manner. Beyond
estimates of status and trend, ensemble methods could be used, for example, to
increase the robustness of spatial predictions when designing networks of
protected areas [@rassweiler2014] or to forecast potential spatial shifts in
species distribution given climate impacts [@harsch2014].

<!-- TODO put back when space: or to evaluate the effect of conservation
management actions when experimental or observational data are limited
[@walsh2012].-->

\noindent

# Acknowledgments

We thank members of Phase I of the working group "Developing new approaches to
global stock status assessment and fishery production potential of the seas"
who contributed to developing the data-limited methods and simulations used in
our analysis. We thank E. Jardim, F. Scott, and J.A. Hutchings for helpful
comments during the development of this project, and R.D. Methot for comments
on an earlier version of the manuscript. We thank the Gordon and Betty Moore
Foundation for funding the working group "Applying data-limited stock status
models and developing management guidance for unassessed fish stocks".

<!--Funding...-->

<!--# Citation notes-->

<!--
- @breiner2015 ensemble models of species distribution models for rare species

- @jones2015 ensemble models of species distribution models - globally for
  marine biodiversity

- [@greene2006] example similar to ours but with climate models (Bayesian
  multilevel ensemble)

- [@kell2007] FLR

- multiple learners book [@alpaydin2010]

- [@caruana2004] nice paper on ensemble models; prob of overfitting increases
  with more models, bagging important

- [@tebaldi2007] key paper: review of 'multi-model ensembles for climate projections'

- famous ensemble is DEMETER: Development of a European Multi-model Ensemble
  System for Seasonal to Interannual Prediction @hagedorn2005 is a good
  reference

- Examples of where ensemble models are shown to be better than any one:
  @thomson2006 (public health - malaria), @cantelaube2005 (agriculture - crop
  yield) (citations taken from @tebaldi2007)

- simple averages are used very commonly in climate science - e.g. IPCC 2001

- @tebaldi2007: weighting obviously makes sense, but how do we define a
  performance metric that is based on past observations that is relevant to the
  future?

- @tebaldi2007: model independence important

- for climate, weighted averages perform better than simple averages [@min2006],
  but are those same weights applicable to the future (or in our case other
  fisheries)?

Why do ensemble methods work?

- error cancellation is one but not the only reason for superiority of ensemble
  models [@hagedorn2005]

- ensemble model may be only marginally better than the best single model in
  any given case, but we don't usually know which is the best single model
  [@hagedorn2005]

- ensemble models useful for regional climate models too; as an example,
  @pierce2009 use 42 metrics to characterize model performance in regional
  climate model ensembles; found that ensemble models were superior to any one
  model --- especially when considering multiple metrics

- @knutti2009: key review paper on motivation and challenges of combining
  climate projection models; performance on testing / current data may only
  weakly relate to future / other datasets

- @murphy2004: Nature paper on ensembles of climate model simulations; weighted
  average better performance than unweighted average

- @dietterich2000: highly cited book chapter "Ensemble Methods in Machine
  Learning"; ensembles are often much more accurate than the "individual
  classifiers that make them up"; but components must be diverse and accurate

- @dietterich2000: a main justification for ensembles: they are
  representational --- no one model usually can contain all hypothetical
  functional forms, but many separate models can cover more hypotheses

- strong paper showing that ensemble models are most accurate when the various
  individual models make errors in uncorrelated ways [@ali1996]

- without substantial training-testing data, appropriate model weights can be
  very hard to deduce and can cause more harm than good (compared to equal
  weighting) [@weigel2010] (they give the example of seasonal forecasting where
  a ton of hind cast testing can be done, vs. long-term climate) (asymmetrical
  loss function)

- [@weigel2010] optimal weights are always as good or better than equal
  weights, but if you get the weights wrong, you can be better off just using
  equal weights; but averaging was almost always better than any one model

- @rykiel1996 "Testing ecological models: the meaning of validation" (see for
  performance criteria, theory on model testing and assessment)

model averaging AIC:
f Chamberlin 1890).  multiple woring hypotheses
  (e.g., Johnson and Omland 2004, Hobbs and Hilborn 2006, Burnham et al. 2011,
  52 Grueber et al. 2011).
-->

<!-- vim: set formatoptions=nroq: -->
