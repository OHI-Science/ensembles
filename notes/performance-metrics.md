Bias vs. accuracy vs. precision

Great review paper: @walther2005 in the context of species richness

@walther2005:
"bias, accuracy, and precision arise in situations of measurement, sampling, and estimation"

IDEA: a figure would be nice -- perhaps the classic dart board, or even along a line as in Fig 1 in @walther2005

@walther2005:
"the difference between a population mean of the measurements or test results and an accepted reference or true value’’ (Bainbridge 1985)."

bias is systematic

@walther2005: Accuracy: 'Accuracy is thus defined as the overall distance between estimated (or observed) values and the true value (Bainbridge 1985, Zar 1996, Jones 1997, Krebs 1999).'

there's probably a nice grid showing these various dimensions:
- measurement, sampling, estimation
- bias, accuracy, precision
- define the intersection of each of the top 3 with each of the bottom 3

See Table 1 in @walterh2005:
- Shows measures of bias, precision and accuracy
- mean error is an example of bias
- 1/variance of error is an example of precision, or 1/CV
- mean squared error is an example of accuracy

ROC (although hard to optimize to - @caruana2004)

## Proposed weights in climate science
- magnitude of bias [@giogi2002]
- observed trends [@greene2006]
- composites of multiple performance measures [@murphy2004] (the ensembles of ensembles Nature paper)


proportional error
absolute proportional error

also called relative error and absolute relative error
@johnson2014 @ono2014 @hurtado-ferro2014 @anderson2014

mean
median

cost functions
not always symmetrical

squared versions: e.g. mean squared error e.g. @hellmann1999

# Retrospective indices
Mohn's rho
@mohn1999
@hurtado-ferro2014
etc.
compares peels to reference (usually recent) year

k as defined in @hurtado-ferro2014
distinguishes convergent and divergent retrospective patterns
adds together differences in relative errors over years
compares peels to truth (not known in true assessments)
